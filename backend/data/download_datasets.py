"""
æ•°æ®é›†ä¸‹è½½å’Œå‡†å¤‡è„šæœ¬
ç”¨äºä¸‹è½½å…¬å¼€æ•°æ®é›†ï¼Œå¹¶è½¬æ¢ä¸ºé¡¹ç›®æ‰€éœ€çš„ CSV æ ¼å¼
"""

import os
import pandas as pd
from datasets import load_dataset

# æ•°æ®ç›®å½•
DATA_DIR = os.path.dirname(os.path.abspath(__file__))


def download_imdb_dataset():
    """
    ä¸‹è½½ IMDB ç”µå½±è¯„è®ºæ•°æ®é›†ï¼ˆæƒ…æ„Ÿåˆ†ç±»ï¼‰
    - æ ‡ç­¾ï¼š0=è´Ÿé¢, 1=æ­£é¢
    """
    print("æ­£åœ¨ä¸‹è½½ IMDB æ•°æ®é›†...")
    
    try:
        dataset = load_dataset("imdb")
        
        # è½¬æ¢ä¸º DataFrame
        train_df = pd.DataFrame({
            "text": dataset["train"]["text"],
            "target": dataset["train"]["label"]
        })
        
        test_df = pd.DataFrame({
            "text": dataset["test"]["text"],
            "target": dataset["test"]["label"]
        })
        
        # ä¿å­˜ä¸º CSV
        train_path = os.path.join(DATA_DIR, "imdb_train.csv")
        test_path = os.path.join(DATA_DIR, "imdb_test.csv")
        
        train_df.to_csv(train_path, index=False, encoding="utf-8")
        test_df.to_csv(test_path, index=False, encoding="utf-8")
        
        print(f"âœ… IMDB è®­ç»ƒé›†å·²ä¿å­˜: {train_path} ({len(train_df)} æ¡)")
        print(f"âœ… IMDB æµ‹è¯•é›†å·²ä¿å­˜: {test_path} ({len(test_df)} æ¡)")
        
        return train_path, test_path
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½ IMDB æ•°æ®é›†å¤±è´¥: {e}")
        return None, None


def download_sst2_dataset():
    """
    ä¸‹è½½ SST-2 æƒ…æ„Ÿåˆ†ææ•°æ®é›†
    - æ ‡ç­¾ï¼š0=è´Ÿé¢, 1=æ­£é¢
    """
    print("æ­£åœ¨ä¸‹è½½ SST-2 æ•°æ®é›†...")
    
    try:
        dataset = load_dataset("glue", "sst2")
        
        train_df = pd.DataFrame({
            "text": dataset["train"]["sentence"],
            "target": dataset["train"]["label"]
        })
        
        val_df = pd.DataFrame({
            "text": dataset["validation"]["sentence"],
            "target": dataset["validation"]["label"]
        })
        
        train_path = os.path.join(DATA_DIR, "sst2_train.csv")
        val_path = os.path.join(DATA_DIR, "sst2_val.csv")
        
        train_df.to_csv(train_path, index=False, encoding="utf-8")
        val_df.to_csv(val_path, index=False, encoding="utf-8")
        
        print(f"âœ… SST-2 è®­ç»ƒé›†å·²ä¿å­˜: {train_path} ({len(train_df)} æ¡)")
        print(f"âœ… SST-2 éªŒè¯é›†å·²ä¿å­˜: {val_path} ({len(val_df)} æ¡)")
        
        return train_path, val_path
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½ SST-2 æ•°æ®é›†å¤±è´¥: {e}")
        return None, None


def download_ag_news_dataset():
    """
    ä¸‹è½½ AG News æ–°é—»åˆ†ç±»æ•°æ®é›†
    - æ ‡ç­¾ï¼š0=World, 1=Sports, 2=Business, 3=Sci/Tech
    """
    print("æ­£åœ¨ä¸‹è½½ AG News æ•°æ®é›†...")
    
    try:
        dataset = load_dataset("ag_news")
        
        train_df = pd.DataFrame({
            "text": dataset["train"]["text"],
            "target": dataset["train"]["label"]
        })
        
        test_df = pd.DataFrame({
            "text": dataset["test"]["text"],
            "target": dataset["test"]["label"]
        })
        
        train_path = os.path.join(DATA_DIR, "ag_news_train.csv")
        test_path = os.path.join(DATA_DIR, "ag_news_test.csv")
        
        train_df.to_csv(train_path, index=False, encoding="utf-8")
        test_df.to_csv(test_path, index=False, encoding="utf-8")
        
        print(f"âœ… AG News è®­ç»ƒé›†å·²ä¿å­˜: {train_path} ({len(train_df)} æ¡)")
        print(f"âœ… AG News æµ‹è¯•é›†å·²ä¿å­˜: {test_path} ({len(test_df)} æ¡)")
        
        return train_path, test_path
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½ AG News æ•°æ®é›†å¤±è´¥: {e}")
        return None, None


def download_chnsenticorp_dataset():
    """
    ä¸‹è½½ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›† ChnSentiCorp
    - æ ‡ç­¾ï¼š0=è´Ÿé¢, 1=æ­£é¢
    """
    print("æ­£åœ¨ä¸‹è½½ ChnSentiCorp ä¸­æ–‡æ•°æ®é›†...")
    
    try:
        # å°è¯•å¤šç§æ•°æ®æº
        dataset = None
        dataset_sources = [
            ("lansinuote/ChnSentiCorp", {}),  # æ–°ç‰ˆæ•°æ®æº
            ("c-s-ale/ChnSentiCorp", {}),     # å¤‡é€‰æ•°æ®æº
            ("seamew/ChnSentiCorp", {"trust_remote_code": True}),  # æ—§ç‰ˆæ•°æ®æºéœ€è¦ä¿¡ä»»ä»£ç 
        ]
        
        for source, kwargs in dataset_sources:
            try:
                print(f"  å°è¯•æ•°æ®æº: {source}")
                dataset = load_dataset(source, **kwargs)
                print(f"  âœ“ æˆåŠŸä» {source} åŠ è½½")
                break
            except Exception as e:
                print(f"  âœ— {source} å¤±è´¥: {str(e)[:50]}...")
                continue
        
        if dataset is None:
            raise Exception("æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è®¿é—®ï¼Œå°è¯•åˆ›å»ºæœ¬åœ°ä¸­æ–‡æ•°æ®é›†")
        
        train_df = pd.DataFrame({
            "text": dataset["train"]["text"],
            "target": dataset["train"]["label"]
        })
        
        val_df = pd.DataFrame({
            "text": dataset["validation"]["text"],
            "target": dataset["validation"]["label"]
        })
        
        test_df = pd.DataFrame({
            "text": dataset["test"]["text"],
            "target": dataset["test"]["label"]
        })
        
        train_path = os.path.join(DATA_DIR, "chnsenticorp_train.csv")
        val_path = os.path.join(DATA_DIR, "chnsenticorp_val.csv")
        test_path = os.path.join(DATA_DIR, "chnsenticorp_test.csv")
        
        train_df.to_csv(train_path, index=False, encoding="utf-8")
        val_df.to_csv(val_path, index=False, encoding="utf-8")
        test_df.to_csv(test_path, index=False, encoding="utf-8")
        
        print(f"âœ… ChnSentiCorp è®­ç»ƒé›†å·²ä¿å­˜: {train_path} ({len(train_df)} æ¡)")
        print(f"âœ… ChnSentiCorp éªŒè¯é›†å·²ä¿å­˜: {val_path} ({len(val_df)} æ¡)")
        print(f"âœ… ChnSentiCorp æµ‹è¯•é›†å·²ä¿å­˜: {test_path} ({len(test_df)} æ¡)")
        
        return train_path, val_path, test_path
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½ ChnSentiCorp æ•°æ®é›†å¤±è´¥: {e}")
        print("ğŸ“ æ­£åœ¨åˆ›å»ºæœ¬åœ°ä¸­æ–‡æƒ…æ„Ÿæ•°æ®é›†...")
        return create_chinese_sentiment_dataset()


def create_chinese_sentiment_dataset():
    """
    åˆ›å»ºæœ¬åœ°ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼ˆå½“åœ¨çº¿æ•°æ®é›†ä¸‹è½½å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
    åŒ…å«çº¦200æ¡ä¸­æ–‡è¯„è®ºæ•°æ®
    """
    print("æ­£åœ¨åˆ›å»ºæœ¬åœ°ä¸­æ–‡æƒ…æ„Ÿæ•°æ®é›†...")
    
    # æ­£é¢è¯„è®º (target=1)
    positive_samples = [
        "è¿™å®¶é…’åº—çš„æœåŠ¡çœŸçš„å¤ªæ£’äº†ï¼Œå‰å°æ€åº¦è¶…å¥½ï¼Œæˆ¿é—´å¹²å‡€æ•´æ´ï¼Œä¸‹æ¬¡è¿˜ä¼šå†æ¥ï¼",
        "ä¹°äº†è¿™æ¬¾æ‰‹æœºï¼Œæ€§èƒ½è¶…å‡ºé¢„æœŸï¼Œæ‹ç…§æ•ˆæœä¸€æµï¼Œç”µæ± ç»­èˆªä¹Ÿå¾ˆç»™åŠ›ã€‚",
        "è¿™éƒ¨ç”µå½±å‰§æƒ…æ„Ÿäººï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œç‰¹æ•ˆä¹Ÿå¾ˆéœ‡æ’¼ï¼Œå¼ºçƒˆæ¨èï¼",
        "é¤å…çš„èœå“å‘³é“é²œç¾ï¼Œç¯å¢ƒä¼˜é›…ï¼ŒæœåŠ¡å‘¨åˆ°ï¼Œæ˜¯çº¦ä¼šçš„å¥½å»å¤„ã€‚",
        "è¿™æœ¬ä¹¦å†™å¾—å¤ªå¥½äº†ï¼Œæƒ…èŠ‚å¼•äººå…¥èƒœï¼Œæ–‡ç¬”ä¼˜ç¾ï¼Œå€¼å¾—ä¸€è¯»å†è¯»ã€‚",
        "å®¢æœæ€åº¦éå¸¸å¥½ï¼Œé—®é¢˜è§£å†³å¾—å¾ˆåŠæ—¶ï¼Œç‰©æµä¹Ÿå¾ˆå¿«ï¼Œå¥½è¯„ï¼",
        "è¿™æ¬¾æŠ¤è‚¤å“ç”¨äº†ä¸€å‘¨ï¼Œçš®è‚¤æ˜æ˜¾å˜å¥½äº†ï¼Œæ€§ä»·æ¯”å¾ˆé«˜ã€‚",
        "é…’åº—ä½ç½®ç»ä½³ï¼Œå‡ºè¡Œæ–¹ä¾¿ï¼Œæ—©é¤ç§ç±»ä¸°å¯Œï¼Œä½å¾—å¾ˆèˆ’å¿ƒã€‚",
        "äº§å“è´¨é‡å¾ˆå¥½ï¼ŒåŒ…è£…ç²¾ç¾ï¼Œé€äººå¾ˆæœ‰é¢å­ï¼Œä¼šå›è´­çš„ã€‚",
        "è¿™å®¶åº—çš„æœåŠ¡æ€åº¦ä¸€çº§æ£’ï¼Œä»·æ ¼å®æƒ ï¼Œä¸œè¥¿ä¹Ÿå¾ˆæ­£å®—ã€‚",
        "æ‰‹æœºè¿è¡Œæµç•…ï¼Œç³»ç»Ÿå¾ˆç¨³å®šï¼Œå¤–è§‚è®¾è®¡ä¹Ÿå¾ˆæ¼‚äº®ã€‚",
        "ç”µå½±é™¢ç¯å¢ƒå¾ˆå¥½ï¼ŒéŸ³æ•ˆéœ‡æ’¼ï¼Œåº§æ¤…èˆ’é€‚ï¼Œè§‚å½±ä½“éªŒæä½³ã€‚",
        "è¿™æ¬¡æ—…è¡Œä½“éªŒéå¸¸æ£’ï¼Œå¯¼æ¸¸ä¸“ä¸šè´Ÿè´£ï¼Œè¡Œç¨‹å®‰æ’åˆç†ã€‚",
        "å•†å“å’Œæè¿°å®Œå…¨ä¸€è‡´ï¼Œå‘è´§é€Ÿåº¦å¿«ï¼ŒåŒ…è£…ä¹Ÿå¾ˆç”¨å¿ƒã€‚",
        "è¯¾ç¨‹å†…å®¹å®ç”¨ï¼Œè€å¸ˆè®²è§£æ¸…æ™°ï¼Œå­¦åˆ°äº†å¾ˆå¤šçŸ¥è¯†ã€‚",
        "è¿™æ¬¾è€³æœºéŸ³è´¨å¤ªèµäº†ï¼Œé™å™ªæ•ˆæœä¸€æµï¼Œæˆ´ç€å¾ˆèˆ’æœã€‚",
        "é¤å…ä¸Šèœé€Ÿåº¦å¿«ï¼Œåˆ†é‡è¶³ï¼Œå£å‘³æ­£å®—ï¼Œæ€§ä»·æ¯”è¶…é«˜ã€‚",
        "é…’åº—è®¾æ–½é½å…¨ï¼Œæˆ¿é—´å®½æ•æ˜äº®ï¼Œæ™¯è§‚ä¹Ÿå¾ˆç¾ã€‚",
        "è¿™ä¸ªAPPç•Œé¢ç®€æ´ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œç”¨èµ·æ¥éå¸¸é¡ºæ‰‹ã€‚",
        "å•†å®¶å‘è´§è¶…å¿«ï¼Œä¸œè¥¿è´¨é‡ä¹Ÿå¾ˆå¥½ï¼Œéå¸¸æ»¡æ„è¿™æ¬¡è´­ç‰©ã€‚",
        "æœåŠ¡äººå‘˜å¾ˆä¸“ä¸šï¼Œè§£ç­”é—®é¢˜è€å¿ƒç»†è‡´ï¼Œä½“éªŒå¾ˆå¥½ã€‚",
        "äº§å“åšå·¥ç²¾ç»†ï¼Œç”¨æ–™æ‰å®ï¼Œç»å¯¹æ˜¯ç‰©è¶…æ‰€å€¼ã€‚",
        "è¿™å®¶é¤å…çš„æ‹›ç‰Œèœå¤ªå¥½åƒäº†ï¼Œä¸‹æ¬¡è¿˜è¦å¸¦æœ‹å‹æ¥ã€‚",
        "å¿«é€’å°å“¥æ€åº¦å¾ˆå¥½ï¼Œé€è´§ä¸Šé—¨å¾ˆåŠæ—¶ï¼Œå¥½è¯„ï¼",
        "è¿™æ¬¾æ¸¸æˆç”»é¢ç²¾ç¾ï¼Œç©æ³•æœ‰è¶£ï¼Œè®©äººçˆ±ä¸é‡Šæ‰‹ã€‚",
        "é…’åº—çš„æ—©é¤å¾ˆä¸°ç››ï¼Œä¸­è¥¿å¼éƒ½æœ‰ï¼Œå‘³é“ä¹Ÿä¸é”™ã€‚",
        "å•†å“æ”¶åˆ°äº†ï¼Œå’Œå–å®¶æè¿°çš„ä¸€æ ·ï¼Œéå¸¸æ»¡æ„ï¼",
        "è¿™æ¬¡å”®åæœåŠ¡ä½“éªŒå¾ˆå¥½ï¼Œé—®é¢˜å¾ˆå¿«å°±è§£å†³äº†ã€‚",
        "äº§å“è®¾è®¡å¾ˆäººæ€§åŒ–ï¼Œä½¿ç”¨æ–¹ä¾¿ï¼ŒçœŸçš„æ˜¯å¥½ç‰©æ¨èã€‚",
        "åº—å®¶æœåŠ¡çƒ­æƒ…ï¼Œå•†å“è´¨é‡å¥½ï¼Œä»·æ ¼ä¹Ÿå…¬é“ã€‚",
        "è¿™æœ¬ä¹¦å†…å®¹ä¸°å¯Œï¼Œè§‚ç‚¹ç‹¬åˆ°ï¼Œè¯»å®Œæ”¶è·å¾ˆå¤§ã€‚",
        "æ‰‹æœºæ‹ç…§æ•ˆæœæƒŠè‰³ï¼Œå¤œæ™¯æ¨¡å¼ç‰¹åˆ«å‡ºè‰²ã€‚",
        "é¤å…ç¯å¢ƒä¼˜ç¾ï¼Œèœå“ç²¾è‡´ï¼Œæ˜¯è¯·å®¢çš„å¥½é€‰æ‹©ã€‚",
        "äº§å“åŒ…è£…ä¸¥å®ï¼Œæ²¡æœ‰ä»»ä½•æŸåï¼Œç‰©æµé€Ÿåº¦ä¹Ÿå¿«ã€‚",
        "è¿™æ¬¾æ´—é¢å¥¶ç”¨ç€å¾ˆæ¸©å’Œï¼Œæ´—å®Œè„¸ä¸ç´§ç»‘ï¼Œå¥½ç”¨ï¼",
        "é…’åº—æœåŠ¡å‘¨åˆ°ï¼Œè®¾æ–½å®Œå–„ï¼Œä½å¾—éå¸¸èˆ’é€‚ã€‚",
        "å•†å“è´¨é‡è¶…å‡ºé¢„æœŸï¼Œè¿™ä¸ªä»·æ ¼ä¹°åˆ°çœŸçš„èµšäº†ã€‚",
        "å®¢æœå›å¤å¾ˆåŠæ—¶ï¼Œæ€åº¦ä¹Ÿå¾ˆå¥½ï¼Œè´­ç‰©ä½“éªŒæ„‰å¿«ã€‚",
        "è¿™éƒ¨å‰§å‰§æƒ…ç´§å‡‘ï¼Œæ¼”å‘˜æ¼”æŠ€ç²¾æ¹›ï¼Œè¿½å‰§åœä¸ä¸‹æ¥ã€‚",
        "äº§å“åŠŸèƒ½é½å…¨ï¼Œæ“ä½œç®€å•ï¼Œè€äººå®¶ä¹Ÿèƒ½è½»æ¾ä½¿ç”¨ã€‚",
        "è¿™å®¶åº—çš„ä¸œè¥¿çœŸçš„å¾ˆå®æƒ ï¼Œå“è´¨ä¹Ÿæœ‰ä¿éšœã€‚",
        "å¿«é€’åŒ…è£…å¾ˆä»”ç»†ï¼Œå•†å“å®Œå¥½æ— æŸï¼Œå¥½è¯„ï¼",
        "é¤å…çš„ç”œç‚¹è¶…çº§å¥½åƒï¼Œç¯å¢ƒä¹Ÿå¾ˆé€‚åˆæ‹ç…§ã€‚",
        "è¿™æ¬¾äº§å“çœŸçš„å¤ªå®ç”¨äº†ï¼Œè§£å†³äº†æˆ‘çš„å¤§é—®é¢˜ã€‚",
        "æœåŠ¡æ€åº¦äº”æ˜Ÿå¥½è¯„ï¼Œé—®é¢˜å¤„ç†å¾—åˆå¿«åˆå¥½ã€‚",
        "å•†å“æ”¶åˆ°å¾ˆæƒŠå–œï¼Œæ¯”å›¾ç‰‡è¿˜å¥½çœ‹ï¼Œæ¨èè´­ä¹°ï¼",
        "é…’åº—åœ°ç†ä½ç½®ä¼˜è¶Šï¼Œäº¤é€šä¾¿åˆ©ï¼Œå‡ºè¡Œå¾ˆæ–¹ä¾¿ã€‚",
        "è¿™ä¸ªå“ç‰Œçš„äº§å“ä¸€ç›´éƒ½å¾ˆå¥½ç”¨ï¼Œä¼šç»§ç»­æ”¯æŒã€‚",
        "è¯¾ç¨‹è®¾ç½®åˆç†ï¼Œå†…å®¹å……å®ï¼Œå­¦ä¹ æ•ˆæœæ˜æ˜¾ã€‚",
        "äº§å“æ€§èƒ½ç¨³å®šï¼Œå”®åæœåŠ¡ä¹Ÿå¾ˆåˆ°ä½ï¼Œå¾ˆæ»¡æ„ã€‚",
    ]
    
    # è´Ÿé¢è¯„è®º (target=0)
    negative_samples = [
        "é…’åº—æˆ¿é—´å¤ªå°äº†ï¼Œéš”éŸ³æ•ˆæœä¹Ÿå·®ï¼Œæ™šä¸Šæ ¹æœ¬ç¡ä¸å¥½ã€‚",
        "è¿™æ¬¾æ‰‹æœºç”¨äº†ä¸€å‘¨å°±å¼€å§‹å¡é¡¿ï¼Œç”µæ± ä¹Ÿä¸è€ç”¨ï¼Œå¤ªå¤±æœ›äº†ã€‚",
        "ç”µå½±å‰§æƒ…æ‹–æ²“ï¼Œç‰¹æ•ˆä¹Ÿå¾ˆå‡ï¼Œå®Œå…¨æ˜¯æµªè´¹æ—¶é—´å’Œé’±ã€‚",
        "é¤å…ä¸Šèœè¶…æ…¢ï¼Œç­‰äº†ä¸€ä¸ªå¤šå°æ—¶ï¼Œèœè¿˜æ˜¯å‡‰çš„ï¼Œå·®è¯„ï¼",
        "è¿™æœ¬ä¹¦å†…å®¹ç©ºæ´ï¼Œæ¯«æ— æ–°æ„ï¼Œå®Œå…¨æ˜¯æµªè´¹é’±ã€‚",
        "å®¢æœæ€åº¦æ¶åŠ£ï¼Œé—®é¢˜ä¸€ç›´æ²¡è§£å†³ï¼Œå†ä¹Ÿä¸ä¼šæ¥äº†ã€‚",
        "æŠ¤è‚¤å“ç”¨äº†è¿‡æ•ï¼Œè´¨é‡å ªå¿§ï¼Œä¸æ•¢å†ç”¨äº†ã€‚",
        "é…’åº—å«ç”Ÿæ¡ä»¶å¤ªå·®ï¼ŒåºŠå•éƒ½æœ‰æ±¡æ¸ï¼Œå¤ªæ¶å¿ƒäº†ã€‚",
        "äº§å“å’Œå›¾ç‰‡å®Œå…¨ä¸ç¬¦ï¼Œè´¨é‡ä¹Ÿå¾ˆå·®ï¼Œè¢«éª—äº†ã€‚",
        "è¿™å®¶åº—æœåŠ¡æ€åº¦æå·®ï¼Œä¸œè¥¿è¿˜è´µï¼Œå†ä¹Ÿä¸æ¥äº†ã€‚",
        "æ‰‹æœºç»å¸¸æ­»æœºé‡å¯ï¼Œç³»ç»Ÿbugå¤ªå¤šï¼Œåæ‚”ä¹°äº†ã€‚",
        "ç”µå½±é™¢åº§ä½å¤ªæŒ¤ï¼Œç©ºè°ƒä¹Ÿä¸å¥½ï¼Œä½“éªŒå¾ˆå·®ã€‚",
        "æ—…è¡Œå›¢å®‰æ’å¤ªç´§å‡‘ï¼Œè´­ç‰©ç‚¹å€’æ˜¯å»äº†ä¸€å¤§å †ã€‚",
        "å•†å“æœ‰æ˜æ˜¾ç‘•ç–µï¼Œå®¢æœè¿˜ä¸ç»™é€€æ¢ï¼Œå¤ªæ°”äººäº†ã€‚",
        "è¯¾ç¨‹å†…å®¹æ°´åˆ†å¤§ï¼Œè€å¸ˆç…§æœ¬å®£ç§‘ï¼Œæµªè´¹é’±ã€‚",
        "è€³æœºéŸ³è´¨ä¸€èˆ¬ï¼Œé™å™ªæ•ˆæœå¾ˆå·®ï¼Œä¸å€¼è¿™ä¸ªä»·ã€‚",
        "é¤å…åˆ†é‡å¤ªå°‘ï¼Œä»·æ ¼è¿˜è´µï¼Œæ€§ä»·æ¯”æä½ã€‚",
        "é…’åº—è®¾æ–½è€æ—§ï¼Œç©ºè°ƒå£°éŸ³å¾ˆå¤§ï¼Œç¡çœ è´¨é‡å·®ã€‚",
        "è¿™ä¸ªAPPå¹¿å‘Šå¤ªå¤šï¼Œè¿˜ç»å¸¸é—ªé€€ï¼Œå¸è½½äº†ã€‚",
        "å•†å®¶å‘è´§æ…¢ï¼Œä¸œè¥¿è¿˜ç ´æŸäº†ï¼Œå”®åæ€åº¦å·®ã€‚",
        "æœåŠ¡äººå‘˜æ€åº¦å†·æ·¡ï¼Œé—®ä»€ä¹ˆéƒ½çˆ±ç­”ä¸ç†çš„ã€‚",
        "äº§å“åšå·¥ç²—ç³™ï¼Œç”¨æ–™å»‰ä»·ï¼Œä¸€ç‚¹éƒ½ä¸å€¼ã€‚",
        "é¤å…èœå“å‘³é“ä¸€èˆ¬ï¼Œç¯å¢ƒä¹Ÿå¾ˆåµé—¹ï¼Œä¸æ¨èã€‚",
        "å¿«é€’æš´åŠ›è¿è¾“ï¼ŒåŒ…è£¹éƒ½å‹å˜å½¢äº†ï¼Œå¤ªä¸è´Ÿè´£äº†ã€‚",
        "æ¸¸æˆbugå¤ªå¤šï¼Œè¿˜ç–¯ç‹‚å……å€¼å¼•å¯¼ï¼Œå·®è¯„ï¼",
        "é…’åº—æ—©é¤å“ç§å°‘ï¼Œå‘³é“ä¹Ÿä¸æ€ä¹ˆæ ·ï¼Œå¤±æœ›ã€‚",
        "å•†å“è´¨é‡å’Œæè¿°ä¸ç¬¦ï¼Œç”³è¯·é€€æ¬¾è¿˜å¾ˆéº»çƒ¦ã€‚",
        "å”®åæœåŠ¡å¤ªå·®äº†ï¼Œæ‰“äº†å¥½å‡ æ¬¡ç”µè¯éƒ½æ²¡äººå¤„ç†ã€‚",
        "äº§å“è®¾è®¡ä¸åˆç†ï¼Œä½¿ç”¨èµ·æ¥å¾ˆä¸æ–¹ä¾¿ã€‚",
        "åº—å®¶æ€åº¦å‚²æ…¢ï¼Œå•†å“æœ‰é—®é¢˜è¿˜ä¸æ‰¿è®¤ã€‚",
        "ä¹¦çš„å°åˆ·è´¨é‡å¾ˆå·®ï¼Œå­—è¿¹æ¨¡ç³Šï¼Œä¸åƒæ­£ç‰ˆã€‚",
        "æ‰‹æœºå‘çƒ­ä¸¥é‡ï¼Œç©ä¸€ä¼šå„¿å°±çƒ«æ‰‹ï¼Œè®¾è®¡æœ‰é—®é¢˜ã€‚",
        "é¤å…æœåŠ¡å‘˜æ€åº¦å·®ï¼Œä¸Šé”™èœäº†è¿˜ä¸é“æ­‰ã€‚",
        "å¿«é€’åŒ…è£…å¤ªç®€é™‹ï¼Œå•†å“éƒ½ç£•ç¢°åäº†ã€‚",
        "æ´—é¢å¥¶ç”¨ç€åˆºæ¿€çš®è‚¤ï¼Œä¸é€‚åˆæ•æ„Ÿè‚Œã€‚",
        "é…’åº—éš”éŸ³å¤ªå·®äº†ï¼Œèµ°å»Šè¯´è¯éƒ½å¬å¾—ä¸€æ¸…äºŒæ¥šã€‚",
        "äº§å“ä»·æ ¼è™šé«˜ï¼Œè´¨é‡å´å¾ˆä¸€èˆ¬ï¼Œä¸å€¼å¾—ä¹°ã€‚",
        "å®¢æœå›å¤æ…¢ï¼Œè¿˜éƒ½æ˜¯æœºå™¨äººå›å¤ï¼Œè§£å†³ä¸äº†é—®é¢˜ã€‚",
        "è¿™éƒ¨å‰§å‰§æƒ…è€å¥—ï¼Œæ¼”æŠ€å°´å°¬ï¼Œå¼ƒå‰§äº†ã€‚",
        "äº§å“åŠŸèƒ½é¸¡è‚‹ï¼Œå®£ä¼ çš„åŠŸèƒ½æ ¹æœ¬ä¸å®ç”¨ã€‚",
        "è¿™å®¶åº—ä¸œè¥¿è´µä¸è¯´ï¼Œè´¨é‡è¿˜æ²¡ä¿éšœã€‚",
        "å¿«é€’å»¶è¯¯ä¸¥é‡ï¼Œå‚¬äº†å¥½å‡ æ¬¡éƒ½æ²¡ç”¨ã€‚",
        "é¤å…ç”œç‚¹å¤ªç”œè…»äº†ï¼Œåƒä¸äº†å‡ å£å°±è…»äº†ã€‚",
        "äº§å“ç”¨äº†å‡ å¤©å°±åäº†ï¼Œè´¨é‡ä¹Ÿå¤ªå·®äº†å§ã€‚",
        "æœåŠ¡æ€åº¦æ•·è¡ï¼Œé—®é¢˜æ‹–äº†å¥½ä¹…éƒ½ä¸è§£å†³ã€‚",
        "å•†å“å›¾ç‰‡å’Œå®ç‰©å·®è·å¤ªå¤§ï¼Œä¸¥é‡è´§ä¸å¯¹æ¿ã€‚",
        "é…’åº—ä½ç½®ååƒ»ï¼Œæ‰“è½¦éƒ½ä¸æ–¹ä¾¿ï¼Œé€‰å€å¤ªå·®ã€‚",
        "è¿™ä¸ªç‰Œå­ä»¥å‰è¿˜è¡Œï¼Œç°åœ¨è´¨é‡è¶Šæ¥è¶Šå·®äº†ã€‚",
        "è¯¾ç¨‹è¿›åº¦å¤ªå¿«ï¼Œæ ¹æœ¬è·Ÿä¸ä¸Šï¼Œä¸é€‚åˆé›¶åŸºç¡€ã€‚",
        "äº§å“å”®åå½¢åŒè™šè®¾ï¼Œåäº†éƒ½æ²¡äººç®¡ã€‚",
    ]
    
    # ç»„åˆæ•°æ®
    all_data = []
    for text in positive_samples:
        all_data.append({"text": text, "target": 1})
    for text in negative_samples:
        all_data.append({"text": text, "target": 0})
    
    df = pd.DataFrame(all_data)
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # æ‰“ä¹±é¡ºåº
    
    # åˆ’åˆ†æ•°æ®é›† (70% è®­ç»ƒ, 15% éªŒè¯, 15% æµ‹è¯•)
    train_size = int(len(df) * 0.7)
    val_size = int(len(df) * 0.15)
    
    train_df = df[:train_size]
    val_df = df[train_size:train_size+val_size]
    test_df = df[train_size+val_size:]
    
    # ä¿å­˜æ–‡ä»¶
    train_path = os.path.join(DATA_DIR, "chinese_sentiment_train.csv")
    val_path = os.path.join(DATA_DIR, "chinese_sentiment_val.csv")
    test_path = os.path.join(DATA_DIR, "chinese_sentiment_test.csv")
    
    train_df.to_csv(train_path, index=False, encoding="utf-8")
    val_df.to_csv(val_path, index=False, encoding="utf-8")
    test_df.to_csv(test_path, index=False, encoding="utf-8")
    
    print(f"âœ… ä¸­æ–‡æƒ…æ„Ÿè®­ç»ƒé›†å·²ä¿å­˜: {train_path} ({len(train_df)} æ¡)")
    print(f"âœ… ä¸­æ–‡æƒ…æ„ŸéªŒè¯é›†å·²ä¿å­˜: {val_path} ({len(val_df)} æ¡)")
    print(f"âœ… ä¸­æ–‡æƒ…æ„Ÿæµ‹è¯•é›†å·²ä¿å­˜: {test_path} ({len(test_df)} æ¡)")
    
    return train_path, val_path, test_path


def create_sample_dataset():
    """
    åˆ›å»ºç¤ºä¾‹æ•°æ®é›†ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    """
    print("æ­£åœ¨åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
    
    sample_data = [
        # æ­£é¢è¯„è®º
        {"text": "è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼Œå‰§æƒ…ç²¾å½©ï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼", "target": 1},
        {"text": "éå¸¸å¥½çš„äº§å“ï¼Œç‰©è¶…æ‰€å€¼ï¼Œå¼ºçƒˆæ¨èï¼", "target": 1},
        {"text": "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œä¸‹æ¬¡è¿˜ä¼šå†æ¥ï¼", "target": 1},
        {"text": "The movie was absolutely fantastic, great acting!", "target": 1},
        {"text": "Excellent product, exactly what I needed.", "target": 1},
        {"text": "Amazing experience, highly recommended!", "target": 1},
        {"text": "è´¨é‡å¾ˆå¥½ï¼Œå‘è´§é€Ÿåº¦å¿«ï¼Œæ»¡æ„ï¼", "target": 1},
        {"text": "Great service and fast delivery!", "target": 1},
        {"text": "è¿™å®¶é¤å…çš„èœå“å‘³é“ä¸€ç»ï¼Œç¯å¢ƒä¹Ÿå¾ˆæ£’ï¼", "target": 1},
        {"text": "The best purchase I've made this year!", "target": 1},
        
        # è´Ÿé¢è¯„è®º
        {"text": "å¤ªå¤±æœ›äº†ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼ã€‚", "target": 0},
        {"text": "è´¨é‡å¤ªå·®ï¼Œç”¨äº†ä¸€å¤©å°±åäº†ã€‚", "target": 0},
        {"text": "æœåŠ¡æ€åº¦æ¶åŠ£ï¼Œå†ä¹Ÿä¸ä¼šæ¥äº†ã€‚", "target": 0},
        {"text": "Terrible movie, waste of time and money.", "target": 0},
        {"text": "Poor quality, broke after one use.", "target": 0},
        {"text": "Awful experience, never coming back.", "target": 0},
        {"text": "åŒ…è£…ç ´æŸï¼Œå•†å“æœ‰ç‘•ç–µï¼Œå¾ˆä¸æ»¡æ„ã€‚", "target": 0},
        {"text": "Shipping was slow and item was damaged.", "target": 0},
        {"text": "è¿™é¤å…åˆè´µåˆéš¾åƒï¼Œç¯å¢ƒä¹Ÿè„ã€‚", "target": 0},
        {"text": "Completely disappointed with this purchase.", "target": 0},
    ]
    
    df = pd.DataFrame(sample_data)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_df = df.sample(frac=0.8, random_state=42)
    test_df = df.drop(train_df.index)
    
    train_path = os.path.join(DATA_DIR, "sample_train.csv")
    test_path = os.path.join(DATA_DIR, "sample_test.csv")
    
    train_df.to_csv(train_path, index=False, encoding="utf-8")
    test_df.to_csv(test_path, index=False, encoding="utf-8")
    
    print(f"âœ… ç¤ºä¾‹è®­ç»ƒé›†å·²ä¿å­˜: {train_path} ({len(train_df)} æ¡)")
    print(f"âœ… ç¤ºä¾‹æµ‹è¯•é›†å·²ä¿å­˜: {test_path} ({len(test_df)} æ¡)")
    
    return train_path, test_path


def list_available_datasets():
    """åˆ—å‡ºå¯ä¸‹è½½çš„æ•°æ®é›†"""
    print("\n" + "="*50)
    print("å¯ç”¨çš„æ•°æ®é›†ä¸‹è½½é€‰é¡¹ï¼š")
    print("="*50)
    print("1. IMDB     - è‹±æ–‡ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ç±» (50,000æ¡)")
    print("2. SST-2    - è‹±æ–‡æƒ…æ„Ÿåˆ†æ (67,000æ¡)")
    print("3. AG News  - è‹±æ–‡æ–°é—»åˆ†ç±» (120,000æ¡)")
    print("4. ChnSentiCorp - ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±» (åœ¨çº¿ä¸‹è½½)")
    print("5. ä¸­æ–‡æƒ…æ„Ÿ   - ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±» (æœ¬åœ°ç”Ÿæˆ, 100æ¡)")
    print("6. Sample   - ç¤ºä¾‹æ•°æ®é›† (20æ¡ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•)")
    print("7. All      - ä¸‹è½½æ‰€æœ‰æ•°æ®é›†")
    print("="*50 + "\n")


if __name__ == "__main__":
    import sys
    
    list_available_datasets()
    
    if len(sys.argv) > 1:
        choice = sys.argv[1].lower()
    else:
        choice = input("è¯·è¾“å…¥é€‰é¡¹ (1-7): ").strip()
    
    if choice in ["1", "imdb"]:
        download_imdb_dataset()
    elif choice in ["2", "sst2"]:
        download_sst2_dataset()
    elif choice in ["3", "ag_news", "agnews"]:
        download_ag_news_dataset()
    elif choice in ["4", "chn", "chnsenticorp"]:
        download_chnsenticorp_dataset()
    elif choice in ["5", "chinese", "cn"]:
        create_chinese_sentiment_dataset()
    elif choice in ["6", "sample"]:
        create_sample_dataset()
    elif choice in ["7", "all"]:
        create_sample_dataset()
        create_chinese_sentiment_dataset()
        download_sst2_dataset()
        download_ag_news_dataset()
        download_chnsenticorp_dataset()
        download_imdb_dataset()
    else:
        print("æ— æ•ˆé€‰é¡¹ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
        create_sample_dataset()
    
    print("\nâœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    print(f"ğŸ“ æ•°æ®ä¿å­˜ç›®å½•: {DATA_DIR}")
