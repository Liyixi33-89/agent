from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import httpx
import os
import json
import asyncio
import threading
import time
from sqlalchemy.orm import Session

from utils_data import load_csv_data, load_json_data, create_data_loader, split_data
from modeling_bert import BertForTextClassification, load_model, load_tokenizer, save_model
from trainer import train_model, Trainer

# å¯¼å…¥æ•°æ®åº“ç›¸å…³æ¨¡å—
from database import get_db, init_db, engine
from db_models import FinetuneTask, ChatHistory, Agent as AgentModel, Model as ModelRecord, TaskStatus
import crud

app = FastAPI(title="Agent å¾®è°ƒå¹³å°")

# é…ç½® CORSï¼Œå…è®¸å‰ç«¯è·¨åŸŸè®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½® Ollama åœ°å€
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# æ•°æ®æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    stream: bool = False
    session_id: Optional[str] = None  # æ–°å¢ï¼šä¼šè¯IDç”¨äºä¿å­˜èŠå¤©å†å²

class FinetuneRequest(BaseModel):
    base_model: str
    dataset_path: str
    new_model_name: str
    epochs: int = 3
    learning_rate: float = 2e-5
    batch_size: int = 8  # å‡å°é»˜è®¤å€¼é¿å…GPUæ˜¾å­˜ä¸è¶³
    max_length: int = 128  # å‡å°é»˜è®¤å€¼é¿å…GPUæ˜¾å­˜ä¸è¶³
    text_column: str = "text"
    label_column: str = "target"
    use_gpu: bool = True  # æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
    gradient_accumulation_steps: int = 4  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç­‰æ•ˆäºæ›´å¤§çš„batch_size

class AgentConfig(BaseModel):
    name: str
    role: str
    system_prompt: str
    model: str
    config: Optional[Dict] = None


# å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“"""
    try:
        init_db()
        print("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿ MySQL å·²å¯åŠ¨å¹¶åˆ›å»ºäº†æ•°æ®åº“ agent_finetune")


@app.get("/")
async def root():
    return {"message": "Agent Finetune Platform API is running"}


# --- GPU çŠ¶æ€æ¥å£ ---

@app.get("/api/gpu/status")
async def get_gpu_status():
    """è·å– GPU çŠ¶æ€ä¿¡æ¯"""
    import torch
    gpu_info = {
        "cuda_available": torch.cuda.is_available(),
        "cuda_version": None,
        "device_count": 0,
        "devices": [],
        "pytorch_version": torch.__version__
    }
    
    if torch.cuda.is_available():
        gpu_info["cuda_version"] = torch.version.cuda
        gpu_info["device_count"] = torch.cuda.device_count()
        for i in range(torch.cuda.device_count()):
            device_props = torch.cuda.get_device_properties(i)
            gpu_info["devices"].append({
                "index": i,
                "name": device_props.name,
                "total_memory_gb": round(device_props.total_memory / (1024**3), 2),
                "major": device_props.major,
                "minor": device_props.minor
            })
    
    return gpu_info


# --- Ollama ä»£ç†æ¥å£ ---

@app.get("/api/models")
async def list_models():
    """è·å– Ollama ä¸­çš„æœ¬åœ°æ¨¡å‹"""
    async with httpx.AsyncClient() as client:
        try:
            resp = await client.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5.0)
            return resp.json()
        except Exception as e:
            # Ollama ä¸å¯ç”¨æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œè€Œä¸æ˜¯æŠ¥é”™
            print(f"âš ï¸ æ— æ³•è¿æ¥ Ollama ({OLLAMA_BASE_URL}): {str(e)}")
            return {"models": [], "error": f"Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å¯åŠ¨ Ollama: ollama serve"}


@app.post("/api/chat")
async def chat(request: ChatRequest, db: Session = Depends(get_db)):
    """ä¸æ¨¡å‹å¯¹è¯"""
    async with httpx.AsyncClient() as client:
        try:
            # è½¬å‘è¯·æ±‚ç»™ Ollama
            ollama_req = {
                "model": request.model,
                "messages": [msg.model_dump() for msg in request.messages],
                "stream": request.stream
            }
            resp = await client.post(f"{OLLAMA_BASE_URL}/api/chat", json=ollama_req, timeout=60.0)
            result = resp.json()
            
            # å¦‚æœæä¾›äº† session_idï¼Œä¿å­˜èŠå¤©å†å²
            if request.session_id:
                # ä¿å­˜ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯
                if request.messages:
                    last_user_msg = request.messages[-1]
                    crud.create_chat_message(
                        db=db,
                        session_id=request.session_id,
                        role=last_user_msg.role,
                        content=last_user_msg.content,
                        model_used=request.model
                    )
                
                # ä¿å­˜åŠ©æ‰‹å›å¤
                if "message" in result:
                    crud.create_chat_message(
                        db=db,
                        session_id=request.session_id,
                        role=result["message"].get("role", "assistant"),
                        content=result["message"].get("content", ""),
                        model_used=request.model
                    )
            
            return result
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")


# --- èŠå¤©å†å²æ¥å£ ---

@app.get("/api/chat/history/{session_id}")
async def get_chat_history(session_id: str, db: Session = Depends(get_db)):
    """è·å–æŒ‡å®šä¼šè¯çš„èŠå¤©å†å²"""
    messages = crud.get_chat_history(db, session_id)
    return [msg.to_dict() for msg in messages]


@app.get("/api/chat/sessions")
async def get_chat_sessions(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰èŠå¤©ä¼šè¯"""
    sessions = crud.get_all_sessions(db)
    return {"sessions": sessions}


@app.delete("/api/chat/history/{session_id}")
async def delete_chat_history(session_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤æŒ‡å®šä¼šè¯çš„èŠå¤©å†å²"""
    count = crud.delete_chat_history(db, session_id)
    return {"deleted": count}


# --- Agent ç®¡ç†æ¥å£ ---

@app.post("/api/agents")
async def create_agent(agent: AgentConfig, db: Session = Depends(get_db)):
    """åˆ›å»ºæ–°çš„ Agent é…ç½®"""
    try:
        db_agent = crud.create_agent(
            db=db,
            name=agent.name,
            role=agent.role,
            system_prompt=agent.system_prompt,
            model=agent.model,
            config=agent.config
        )
        return {"status": "success", "agent": db_agent.to_dict()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"åˆ›å»º Agent å¤±è´¥: {str(e)}")


@app.get("/api/agents")
async def get_agents(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰ Agent"""
    agents = crud.get_all_agents(db)
    return [agent.to_dict() for agent in agents]


@app.get("/api/agents/{agent_id}")
async def get_agent(agent_id: str, db: Session = Depends(get_db)):
    """è·å–æŒ‡å®š Agent"""
    agent = crud.get_agent(db, agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")
    return agent.to_dict()


@app.put("/api/agents/{agent_id}")
async def update_agent(agent_id: str, agent: AgentConfig, db: Session = Depends(get_db)):
    """æ›´æ–° Agent"""
    updated_agent = crud.update_agent(
        db=db,
        agent_id=agent_id,
        name=agent.name,
        role=agent.role,
        system_prompt=agent.system_prompt,
        model=agent.model,
        config=agent.config
    )
    if not updated_agent:
        raise HTTPException(status_code=404, detail="Agent not found")
    return {"status": "success", "agent": updated_agent.to_dict()}


@app.delete("/api/agents/{agent_id}")
async def delete_agent(agent_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤ Agent"""
    success = crud.delete_agent(db, agent_id)
    if not success:
        raise HTTPException(status_code=404, detail="Agent not found")
    return {"status": "deleted"}


# --- å¾®è°ƒç›¸å…³æ¥å£ ---

def run_finetune_task_sync(task_id: str, req: FinetuneRequest):
    """åŒæ­¥è¿è¡Œå¾®è°ƒä»»åŠ¡"""
    import torch
    # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯ï¼ˆå› ä¸ºåœ¨çº¿ç¨‹ä¸­ï¼‰
    from database import SessionLocal
    db = SessionLocal()
    
    # æ ¹æ®é…ç½®å’Œç¡¬ä»¶æƒ…å†µå†³å®šä½¿ç”¨çš„è®¾å¤‡
    if req.use_gpu and torch.cuda.is_available():
        device = "cuda"
        print(f"ğŸš€ ä½¿ç”¨ GPU è®­ç»ƒ: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        if req.use_gpu and not torch.cuda.is_available():
            print("âš ï¸ è¯·æ±‚ä½¿ç”¨ GPU ä½† CUDA ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU è®­ç»ƒ")
        else:
            print("ğŸ“Œ ä½¿ç”¨ CPU è®­ç»ƒ")
    
    # è¿›åº¦å›è°ƒå‡½æ•°
    def progress_callback(current_epoch: int, total_epochs: int, progress: float):
        """æ›´æ–°è®­ç»ƒè¿›åº¦åˆ°æ•°æ®åº“"""
        try:
            crud.update_finetune_task_status(
                db=db,
                task_id=task_id,
                status=TaskStatus.RUNNING.value,
                progress=progress
            )
            print(f"Task {task_id}: Epoch {current_epoch}/{total_epochs}, Progress: {progress:.1f}%")
        except Exception as e:
            print(f"Error updating progress: {e}")
    
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        crud.update_finetune_task_status(db, task_id, TaskStatus.RUNNING.value, progress=0.0)
        
        print(f"Starting finetune task {task_id} for model {req.new_model_name}...")
        
        # åŠ è½½æ•°æ®
        if req.dataset_path.endswith('.csv'):
            texts, labels = load_csv_data(req.dataset_path, req.text_column, req.label_column)
        elif req.dataset_path.endswith('.json'):
            texts, labels = load_json_data(req.dataset_path, req.text_column, req.label_column)
        else:
            raise ValueError("Unsupported file format. Use CSV or JSON.")
        
        # åˆ’åˆ†æ•°æ®é›†
        train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = split_data(
            texts, labels, train_ratio=0.8, val_ratio=0.1
        )
        
        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        tokenizer = load_tokenizer(req.base_model)
        model = load_model(req.base_model, num_labels=len(set(labels)))
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = create_data_loader(train_texts, train_labels, tokenizer, 
                                         batch_size=req.batch_size, max_length=req.max_length, shuffle=True)
        val_loader = create_data_loader(val_texts, val_labels, tokenizer,
                                      batch_size=req.batch_size, max_length=req.max_length)
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦è¿›åº¦å›è°ƒã€è®¾å¤‡é…ç½®å’Œæ˜¾å­˜ä¼˜åŒ–ï¼‰
        history = train_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=req.epochs,
            learning_rate=req.learning_rate,
            progress_callback=progress_callback,
            device=device,  # ä½¿ç”¨é…ç½®çš„è®¾å¤‡
            gradient_accumulation_steps=req.gradient_accumulation_steps,  # æ¢¯åº¦ç´¯ç§¯
            use_amp=(device == "cuda")  # GPUæ—¶å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        )
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/{req.new_model_name}.pth"
        os.makedirs("models", exist_ok=True)
        save_model(model, model_path)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.COMPLETED.value,
            model_path=model_path,
            training_history=history,
            progress=100.0
        )
        
        # åˆ›å»ºæ¨¡å‹è®°å½•
        crud.create_model(
            db=db,
            name=req.new_model_name,
            model_type="finetuned",
            base_model=req.base_model,
            path=model_path,
            description=f"ä» {req.base_model} å¾®è°ƒå¾—åˆ°",
            finetune_task_id=task_id
        )
        
        print(f"Finetune task {task_id} completed. Model saved to {model_path}")
        
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.FAILED.value,
            error_message=str(e)
        )
        print(f"Finetune task {task_id} failed: {str(e)}")
    finally:
        db.close()


async def run_finetune_task(task_id: str, req: FinetuneRequest):
    """å¼‚æ­¥è¿è¡Œå¾®è°ƒä»»åŠ¡"""
    # åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ä»»åŠ¡
    thread = threading.Thread(target=run_finetune_task_sync, args=(task_id, req))
    thread.start()


@app.post("/api/finetune")
async def start_finetune(req: FinetuneRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """å¯åŠ¨å¾®è°ƒä»»åŠ¡"""
    # åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“
    task = crud.create_finetune_task(
        db=db,
        base_model=req.base_model,
        new_model_name=req.new_model_name,
        dataset_path=req.dataset_path,
        epochs=req.epochs,
        learning_rate=req.learning_rate,
        batch_size=req.batch_size,
        max_length=req.max_length,
        gradient_accumulation_steps=req.gradient_accumulation_steps,
        text_column=req.text_column,
        label_column=req.label_column,
        use_gpu=req.use_gpu
    )
    
    task_id = str(task.id)
    
    # åœ¨åå°è¿è¡Œå¾®è°ƒä»»åŠ¡
    background_tasks.add_task(run_finetune_task, task_id, req)
    
    return {"task_id": task_id, "status": "started"}


@app.get("/api/finetune/{task_id}")
async def get_finetune_status(task_id: str, db: Session = Depends(get_db)):
    """è·å–å¾®è°ƒä»»åŠ¡çŠ¶æ€"""
    task = crud.get_finetune_task(db, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return task.to_dict()


@app.get("/api/finetune")
async def list_finetune_tasks(
    status: Optional[str] = None,
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(get_db)
):
    """è·å–æ‰€æœ‰å¾®è°ƒä»»åŠ¡åˆ—è¡¨"""
    tasks = crud.get_all_finetune_tasks(db, skip=skip, limit=limit, status=status)
    return [task.to_dict() for task in tasks]


@app.delete("/api/finetune/{task_id}")
async def delete_finetune_task(task_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤å¾®è°ƒä»»åŠ¡"""
    success = crud.delete_finetune_task(db, task_id)
    if not success:
        raise HTTPException(status_code=404, detail="Task not found")
    return {"status": "deleted"}


# --- æ¨¡å‹ç®¡ç†æ¥å£ ---

@app.get("/api/models/finetuned")
async def list_finetuned_models(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰å¾®è°ƒåçš„æ¨¡å‹"""
    models = crud.get_all_models(db, model_type="finetuned")
    return [model.to_dict() for model in models]


# --- æ¨¡å‹é¢„æµ‹æ¥å£ ---

class PredictRequest(BaseModel):
    model_path: str
    text: str
    base_model: str = "bert-base-uncased"

@app.post("/api/models/predict")
async def predict_with_model(req: PredictRequest):
    """ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    import torch
    from modeling_bert import load_saved_model
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(req.model_path):
            raise HTTPException(status_code=404, detail=f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {req.model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = load_tokenizer(req.base_model)
        
        # ä½¿ç”¨ load_saved_model å‡½æ•°åŠ è½½æ¨¡å‹ï¼ˆå®ƒèƒ½æ­£ç¡®å¤„ç†ä¿å­˜çš„å­—å…¸æ ¼å¼ï¼‰
        model = load_saved_model(req.model_path, device='cpu')
        model.eval()
        
        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç 
        encoding = tokenizer(
            req.text,
            truncation=True,
            max_length=512,
            padding='max_length',
            return_tensors='pt'
        )
        
        # è¿›è¡Œé¢„æµ‹
        with torch.no_grad():
            outputs = model(
                input_ids=encoding['input_ids'],
                attention_mask=encoding['attention_mask']
            )
            # æ¨¡å‹è¿”å›çš„æ˜¯å­—å…¸ï¼ŒåŒ…å« 'logits' é”®
            logits = outputs['logits'] if isinstance(outputs, dict) else outputs
            probabilities = torch.softmax(logits, dim=1)
            prediction = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][prediction].item()
        
        return {
            "text": req.text,
            "prediction": prediction,
            "confidence": confidence,
            "probabilities": probabilities[0].tolist()
        }
        
    except Exception as e:
        import traceback
        error_detail = f"é¢„æµ‹å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        print(error_detail)
        raise HTTPException(status_code=500, detail=f"é¢„æµ‹å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
