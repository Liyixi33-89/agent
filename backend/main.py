from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, UploadFile, File, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Set
from contextlib import asynccontextmanager
import httpx
import os
import json
import asyncio
import threading
import time
import logging
import shutil
from datetime import datetime
from sqlalchemy.orm import Session

from utils_data import load_csv_data, load_json_data, create_data_loader, split_data
from modeling_bert import BertForTextClassification, load_model, load_tokenizer, save_model
from trainer import train_model, Trainer

# å¯¼å…¥æ•°æ®åº“ç›¸å…³æ¨¡å—
from database import get_db, init_db, engine
from db_models import FinetuneTask, ChatHistory, Agent as AgentModel, Model as ModelRecord, TaskStatus
import crud

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å­˜å‚¨è¿è¡Œä¸­çš„ä»»åŠ¡çº¿ç¨‹ï¼Œç”¨äºå–æ¶ˆåŠŸèƒ½
running_tasks: Dict[str, threading.Thread] = {}
task_cancel_flags: Dict[str, bool] = {}

# WebSocket è¿æ¥ç®¡ç†
class ConnectionManager:
    """WebSocket è¿æ¥ç®¡ç†å™¨"""
    def __init__(self):
        # task_id -> set of websocket connections
        self.active_connections: Dict[str, Set[WebSocket]] = {}
    
    async def connect(self, websocket: WebSocket, task_id: str):
        await websocket.accept()
        if task_id not in self.active_connections:
            self.active_connections[task_id] = set()
        self.active_connections[task_id].add(websocket)
        logger.info(f"WebSocket connected for task {task_id}")
    
    def disconnect(self, websocket: WebSocket, task_id: str):
        if task_id in self.active_connections:
            self.active_connections[task_id].discard(websocket)
            if not self.active_connections[task_id]:
                del self.active_connections[task_id]
        logger.info(f"WebSocket disconnected for task {task_id}")
    
    async def send_log(self, task_id: str, message: str, level: str = "info"):
        """å‘é€æ—¥å¿—æ¶ˆæ¯åˆ°æ‰€æœ‰è®¢é˜…è¯¥ä»»åŠ¡çš„å®¢æˆ·ç«¯"""
        if task_id in self.active_connections:
            log_data = {
                "type": "log",
                "task_id": task_id,
                "level": level,
                "message": message,
                "timestamp": datetime.utcnow().isoformat()
            }
            dead_connections = set()
            for connection in self.active_connections[task_id]:
                try:
                    await connection.send_json(log_data)
                except Exception:
                    dead_connections.add(connection)
            # æ¸…ç†æ–­å¼€çš„è¿æ¥
            for conn in dead_connections:
                self.active_connections[task_id].discard(conn)
    
    async def send_progress(self, task_id: str, progress: float, epoch: int, total_epochs: int):
        """å‘é€è¿›åº¦æ›´æ–°"""
        if task_id in self.active_connections:
            progress_data = {
                "type": "progress",
                "task_id": task_id,
                "progress": progress,
                "epoch": epoch,
                "total_epochs": total_epochs,
                "timestamp": datetime.utcnow().isoformat()
            }
            dead_connections = set()
            for connection in self.active_connections[task_id]:
                try:
                    await connection.send_json(progress_data)
                except Exception:
                    dead_connections.add(connection)
            for conn in dead_connections:
                self.active_connections[task_id].discard(conn)

# å…¨å±€ WebSocket ç®¡ç†å™¨
ws_manager = ConnectionManager()

# åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨å¯åŠ¨å’Œå…³é—­æ—¶çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    try:
        init_db()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âš ï¸ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        logger.error("è¯·ç¡®ä¿ MySQL å·²å¯åŠ¨å¹¶åˆ›å»ºäº†æ•°æ®åº“ agent_finetune")
    
    yield  # åº”ç”¨è¿è¡Œä¸­
    
    # å…³é—­æ—¶æ¸…ç†
    logger.info("ğŸ”„ åº”ç”¨æ­£åœ¨å…³é—­ï¼Œæ¸…ç†èµ„æº...")
    # å–æ¶ˆæ‰€æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
    for task_id in list(task_cancel_flags.keys()):
        task_cancel_flags[task_id] = True
    logger.info("âœ… åº”ç”¨å·²å…³é—­")

app = FastAPI(title="Agent å¾®è°ƒå¹³å°", lifespan=lifespan)

# é…ç½® CORSï¼Œå…è®¸å‰ç«¯è·¨åŸŸè®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½® Ollama åœ°å€
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# æ•°æ®æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    stream: bool = False
    session_id: Optional[str] = None  # æ–°å¢ï¼šä¼šè¯IDç”¨äºä¿å­˜èŠå¤©å†å²

class FinetuneRequest(BaseModel):
    base_model: str
    dataset_path: str
    new_model_name: str
    epochs: int = Field(default=3, ge=1, le=100, description="è®­ç»ƒè½®æ•°")
    learning_rate: float = Field(default=2e-5, gt=0, description="å­¦ä¹ ç‡")
    batch_size: int = Field(default=8, ge=1, le=64, description="æ‰¹æ¬¡å¤§å°")  # å‡å°é»˜è®¤å€¼é¿å…GPUæ˜¾å­˜ä¸è¶³
    max_length: int = Field(default=128, ge=32, le=512, description="æœ€å¤§åºåˆ—é•¿åº¦")  # å‡å°é»˜è®¤å€¼é¿å…GPUæ˜¾å­˜ä¸è¶³
    text_column: str = "text"
    label_column: str = "target"
    use_gpu: bool = True  # æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
    gradient_accumulation_steps: int = Field(default=4, ge=1, le=32, description="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç­‰æ•ˆäºæ›´å¤§çš„batch_size

class AgentConfig(BaseModel):
    name: str
    role: str
    system_prompt: str
    model: str
    config: Optional[Dict] = None


@app.get("/")
async def root():
    return {"message": "Agent Finetune Platform API is running"}


# --- GPU çŠ¶æ€æ¥å£ ---

@app.get("/api/gpu/status")
async def get_gpu_status():
    """è·å– GPU çŠ¶æ€ä¿¡æ¯"""
    import torch
    gpu_info = {
        "cuda_available": torch.cuda.is_available(),
        "cuda_version": None,
        "device_count": 0,
        "devices": [],
        "pytorch_version": torch.__version__
    }
    
    if torch.cuda.is_available():
        gpu_info["cuda_version"] = torch.version.cuda
        gpu_info["device_count"] = torch.cuda.device_count()
        for i in range(torch.cuda.device_count()):
            device_props = torch.cuda.get_device_properties(i)
            gpu_info["devices"].append({
                "index": i,
                "name": device_props.name,
                "total_memory_gb": round(device_props.total_memory / (1024**3), 2),
                "major": device_props.major,
                "minor": device_props.minor
            })
    
    return gpu_info


# --- Ollama ä»£ç†æ¥å£ ---

@app.get("/api/models")
async def list_models():
    """è·å– Ollama ä¸­çš„æœ¬åœ°æ¨¡å‹"""
    async with httpx.AsyncClient() as client:
        try:
            resp = await client.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5.0)
            return resp.json()
        except Exception as e:
            # Ollama ä¸å¯ç”¨æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œè€Œä¸æ˜¯æŠ¥é”™
            print(f"âš ï¸ æ— æ³•è¿æ¥ Ollama ({OLLAMA_BASE_URL}): {str(e)}")
            return {"models": [], "error": f"Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å¯åŠ¨ Ollama: ollama serve"}


@app.post("/api/chat")
async def chat(request: ChatRequest, db: Session = Depends(get_db)):
    """ä¸æ¨¡å‹å¯¹è¯"""
    async with httpx.AsyncClient() as client:
        try:
            # è½¬å‘è¯·æ±‚ç»™ Ollama
            ollama_req = {
                "model": request.model,
                "messages": [msg.model_dump() for msg in request.messages],
                "stream": request.stream
            }
            resp = await client.post(f"{OLLAMA_BASE_URL}/api/chat", json=ollama_req, timeout=60.0)
            result = resp.json()
            
            # å¦‚æœæä¾›äº† session_idï¼Œä¿å­˜èŠå¤©å†å²
            if request.session_id:
                # ä¿å­˜ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯
                if request.messages:
                    last_user_msg = request.messages[-1]
                    crud.create_chat_message(
                        db=db,
                        session_id=request.session_id,
                        role=last_user_msg.role,
                        content=last_user_msg.content,
                        model_used=request.model
                    )
                
                # ä¿å­˜åŠ©æ‰‹å›å¤
                if "message" in result:
                    crud.create_chat_message(
                        db=db,
                        session_id=request.session_id,
                        role=result["message"].get("role", "assistant"),
                        content=result["message"].get("content", ""),
                        model_used=request.model
                    )
            
            return result
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")


# --- èŠå¤©å†å²æ¥å£ ---

@app.get("/api/chat/history/{session_id}")
async def get_chat_history(session_id: str, db: Session = Depends(get_db)):
    """è·å–æŒ‡å®šä¼šè¯çš„èŠå¤©å†å²"""
    messages = crud.get_chat_history(db, session_id)
    return [msg.to_dict() for msg in messages]


@app.get("/api/chat/sessions")
async def get_chat_sessions(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰èŠå¤©ä¼šè¯"""
    sessions = crud.get_all_sessions(db)
    return {"sessions": sessions}


@app.delete("/api/chat/history/{session_id}")
async def delete_chat_history(session_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤æŒ‡å®šä¼šè¯çš„èŠå¤©å†å²"""
    count = crud.delete_chat_history(db, session_id)
    return {"deleted": count}


# --- Agent ç®¡ç†æ¥å£ ---

@app.post("/api/agents")
async def create_agent(agent: AgentConfig, db: Session = Depends(get_db)):
    """åˆ›å»ºæ–°çš„ Agent é…ç½®"""
    try:
        db_agent = crud.create_agent(
            db=db,
            name=agent.name,
            role=agent.role,
            system_prompt=agent.system_prompt,
            model=agent.model,
            config=agent.config
        )
        return {"status": "success", "agent": db_agent.to_dict()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"åˆ›å»º Agent å¤±è´¥: {str(e)}")


@app.get("/api/agents")
async def get_agents(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰ Agent"""
    agents = crud.get_all_agents(db)
    return [agent.to_dict() for agent in agents]


@app.get("/api/agents/{agent_id}")
async def get_agent(agent_id: str, db: Session = Depends(get_db)):
    """è·å–æŒ‡å®š Agent"""
    agent = crud.get_agent(db, agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")
    return agent.to_dict()


@app.put("/api/agents/{agent_id}")
async def update_agent(agent_id: str, agent: AgentConfig, db: Session = Depends(get_db)):
    """æ›´æ–° Agent"""
    updated_agent = crud.update_agent(
        db=db,
        agent_id=agent_id,
        name=agent.name,
        role=agent.role,
        system_prompt=agent.system_prompt,
        model=agent.model,
        config=agent.config
    )
    if not updated_agent:
        raise HTTPException(status_code=404, detail="Agent not found")
    return {"status": "success", "agent": updated_agent.to_dict()}


@app.delete("/api/agents/{agent_id}")
async def delete_agent(agent_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤ Agent"""
    success = crud.delete_agent(db, agent_id)
    if not success:
        raise HTTPException(status_code=404, detail="Agent not found")
    return {"status": "deleted"}


# --- å¾®è°ƒç›¸å…³æ¥å£ ---

def run_finetune_task_sync(task_id: str, req: FinetuneRequest):
    """åŒæ­¥è¿è¡Œå¾®è°ƒä»»åŠ¡"""
    import torch
    # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯ï¼ˆå› ä¸ºåœ¨çº¿ç¨‹ä¸­ï¼‰
    from database import SessionLocal
    db = SessionLocal()
    
    # åˆå§‹åŒ–å–æ¶ˆæ ‡å¿—
    task_cancel_flags[task_id] = False
    
    # æ ¹æ®é…ç½®å’Œç¡¬ä»¶æƒ…å†µå†³å®šä½¿ç”¨çš„è®¾å¤‡
    if req.use_gpu and torch.cuda.is_available():
        device = "cuda"
        logger.info(f"ğŸš€ ä½¿ç”¨ GPU è®­ç»ƒ: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        if req.use_gpu and not torch.cuda.is_available():
            logger.warning("âš ï¸ è¯·æ±‚ä½¿ç”¨ GPU ä½† CUDA ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU è®­ç»ƒ")
        else:
            logger.info("ğŸ“Œ ä½¿ç”¨ CPU è®­ç»ƒ")
    
    # è¿›åº¦å›è°ƒå‡½æ•°
    def progress_callback(current_epoch: int, total_epochs: int, progress: float):
        """æ›´æ–°è®­ç»ƒè¿›åº¦åˆ°æ•°æ®åº“"""
        # æ£€æŸ¥å–æ¶ˆæ ‡å¿—
        if task_cancel_flags.get(task_id, False):
            raise InterruptedError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
        
        try:
            crud.update_finetune_task_status(
                db=db,
                task_id=task_id,
                status=TaskStatus.RUNNING.value,
                progress=progress
            )
            logger.info(f"Task {task_id}: Epoch {current_epoch}/{total_epochs}, Progress: {progress:.1f}%")
        except Exception as e:
            logger.error(f"Error updating progress: {e}")
    
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        crud.update_finetune_task_status(db, task_id, TaskStatus.RUNNING.value, progress=0.0)
        
        logger.info(f"Starting finetune task {task_id} for model {req.new_model_name}...")
        
        # åŠ è½½æ•°æ®
        if req.dataset_path.endswith('.csv'):
            texts, labels = load_csv_data(req.dataset_path, req.text_column, req.label_column)
        elif req.dataset_path.endswith('.json'):
            texts, labels = load_json_data(req.dataset_path, req.text_column, req.label_column)
        else:
            raise ValueError("Unsupported file format. Use CSV or JSON.")
        
        # åˆ’åˆ†æ•°æ®é›†
        train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = split_data(
            texts, labels, train_ratio=0.8, val_ratio=0.1
        )
        
        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        tokenizer = load_tokenizer(req.base_model)
        model = load_model(req.base_model, num_labels=len(set(labels)))
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = create_data_loader(train_texts, train_labels, tokenizer, 
                                         batch_size=req.batch_size, max_length=req.max_length, shuffle=True)
        val_loader = create_data_loader(val_texts, val_labels, tokenizer,
                                      batch_size=req.batch_size, max_length=req.max_length)
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦è¿›åº¦å›è°ƒã€è®¾å¤‡é…ç½®å’Œæ˜¾å­˜ä¼˜åŒ–ï¼‰
        history = train_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=req.epochs,
            learning_rate=req.learning_rate,
            progress_callback=progress_callback,
            device=device,  # ä½¿ç”¨é…ç½®çš„è®¾å¤‡
            gradient_accumulation_steps=req.gradient_accumulation_steps,  # æ¢¯åº¦ç´¯ç§¯
            use_amp=(device == "cuda")  # GPUæ—¶å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        )
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/{req.new_model_name}.pth"
        os.makedirs("models", exist_ok=True)
        save_model(model, model_path)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.COMPLETED.value,
            model_path=model_path,
            training_history=history,
            progress=100.0
        )
        
        # åˆ›å»ºæ¨¡å‹è®°å½•
        crud.create_model(
            db=db,
            name=req.new_model_name,
            model_type="finetuned",
            base_model=req.base_model,
            path=model_path,
            description=f"ä» {req.base_model} å¾®è°ƒå¾—åˆ°",
            finetune_task_id=task_id
        )
        
        logger.info(f"Finetune task {task_id} completed. Model saved to {model_path}")
        
    except InterruptedError as e:
        # ä»»åŠ¡è¢«å–æ¶ˆ
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.CANCELLED.value,
            error_message=str(e)
        )
        logger.warning(f"Finetune task {task_id} cancelled: {str(e)}")
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        import traceback
        error_detail = f"{str(e)}\n{traceback.format_exc()}"
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.FAILED.value,
            error_message=error_detail
        )
        logger.error(f"Finetune task {task_id} failed: {str(e)}")
    finally:
        # æ¸…ç†å–æ¶ˆæ ‡å¿—
        if task_id in task_cancel_flags:
            del task_cancel_flags[task_id]
        if task_id in running_tasks:
            del running_tasks[task_id]
        db.close()


async def run_finetune_task(task_id: str, req: FinetuneRequest):
    """å¼‚æ­¥è¿è¡Œå¾®è°ƒä»»åŠ¡"""
    # åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ä»»åŠ¡
    thread = threading.Thread(target=run_finetune_task_sync, args=(task_id, req))
    thread.start()
    # è®°å½•è¿è¡Œä¸­çš„ä»»åŠ¡
    running_tasks[task_id] = thread


@app.post("/api/finetune")
async def start_finetune(req: FinetuneRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """å¯åŠ¨å¾®è°ƒä»»åŠ¡"""
    # åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“
    task = crud.create_finetune_task(
        db=db,
        base_model=req.base_model,
        new_model_name=req.new_model_name,
        dataset_path=req.dataset_path,
        epochs=req.epochs,
        learning_rate=req.learning_rate,
        batch_size=req.batch_size,
        max_length=req.max_length,
        gradient_accumulation_steps=req.gradient_accumulation_steps,
        text_column=req.text_column,
        label_column=req.label_column,
        use_gpu=req.use_gpu
    )
    
    task_id = str(task.id)
    
    # åœ¨åå°è¿è¡Œå¾®è°ƒä»»åŠ¡
    background_tasks.add_task(run_finetune_task, task_id, req)
    
    return {"task_id": task_id, "status": "started"}


@app.get("/api/finetune/{task_id}")
async def get_finetune_status(task_id: str, db: Session = Depends(get_db)):
    """è·å–å¾®è°ƒä»»åŠ¡çŠ¶æ€"""
    task = crud.get_finetune_task(db, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return task.to_dict()


@app.get("/api/finetune")
async def list_finetune_tasks(
    status: Optional[str] = None,
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(get_db)
):
    """è·å–æ‰€æœ‰å¾®è°ƒä»»åŠ¡åˆ—è¡¨"""
    tasks = crud.get_all_finetune_tasks(db, skip=skip, limit=limit, status=status)
    return [task.to_dict() for task in tasks]


@app.delete("/api/finetune/{task_id}")
async def delete_finetune_task(task_id: str, db: Session = Depends(get_db)):
    """åˆ é™¤å¾®è°ƒä»»åŠ¡"""
    success = crud.delete_finetune_task(db, task_id)
    if not success:
        raise HTTPException(status_code=404, detail="Task not found")
    return {"status": "deleted"}


@app.post("/api/finetune/{task_id}/cancel")
async def cancel_finetune_task(task_id: str, db: Session = Depends(get_db)):
    """å–æ¶ˆæ­£åœ¨è¿è¡Œçš„å¾®è°ƒä»»åŠ¡"""
    task = crud.get_finetune_task(db, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    if task.status not in [TaskStatus.RUNNING.value, TaskStatus.PENDING.value]:
        raise HTTPException(status_code=400, detail=f"æ— æ³•å–æ¶ˆçŠ¶æ€ä¸º {task.status} çš„ä»»åŠ¡")
    
    # è®¾ç½®å–æ¶ˆæ ‡å¿—
    if task_id in task_cancel_flags:
        task_cancel_flags[task_id] = True
        logger.info(f"ä»»åŠ¡ {task_id} å·²æ ‡è®°ä¸ºå–æ¶ˆ")
        return {"status": "cancelling", "message": "ä»»åŠ¡æ­£åœ¨å–æ¶ˆä¸­ï¼Œè¯·ç¨å€™..."}
    else:
        # ä»»åŠ¡å¯èƒ½è¿˜æœªå¼€å§‹è¿è¡Œï¼Œç›´æ¥æ›´æ–°çŠ¶æ€
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.CANCELLED.value,
            error_message="ä»»åŠ¡åœ¨å¯åŠ¨å‰è¢«å–æ¶ˆ"
        )
        return {"status": "cancelled", "message": "ä»»åŠ¡å·²å–æ¶ˆ"}


# --- æ¨¡å‹ç®¡ç†æ¥å£ ---

@app.get("/api/models/finetuned")
async def list_finetuned_models(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰å¾®è°ƒåçš„æ¨¡å‹"""
    models = crud.get_all_models(db, model_type="finetuned")
    return [model.to_dict() for model in models]


# --- æ¨¡å‹é¢„æµ‹æ¥å£ ---

class PredictRequest(BaseModel):
    model_path: str
    text: str
    base_model: str = "bert-base-uncased"

@app.post("/api/models/predict")
async def predict_with_model(req: PredictRequest):
    """ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    import torch
    from modeling_bert import load_saved_model
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(req.model_path):
            raise HTTPException(status_code=404, detail=f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {req.model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = load_tokenizer(req.base_model)
        
        # ä½¿ç”¨ load_saved_model å‡½æ•°åŠ è½½æ¨¡å‹ï¼ˆå®ƒèƒ½æ­£ç¡®å¤„ç†ä¿å­˜çš„å­—å…¸æ ¼å¼ï¼‰
        model = load_saved_model(req.model_path, device='cpu')
        model.eval()
        
        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç 
        encoding = tokenizer(
            req.text,
            truncation=True,
            max_length=512,
            padding='max_length',
            return_tensors='pt'
        )
        
        # è¿›è¡Œé¢„æµ‹
        with torch.no_grad():
            outputs = model(
                input_ids=encoding['input_ids'],
                attention_mask=encoding['attention_mask']
            )
            # æ¨¡å‹è¿”å›çš„æ˜¯å­—å…¸ï¼ŒåŒ…å« 'logits' é”®
            logits = outputs['logits'] if isinstance(outputs, dict) else outputs
            probabilities = torch.softmax(logits, dim=1)
            prediction = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][prediction].item()
        
        return {
            "text": req.text,
            "prediction": prediction,
            "confidence": confidence,
            "probabilities": probabilities[0].tolist()
        }
        
    except Exception as e:
        import traceback
        error_detail = f"é¢„æµ‹å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        print(error_detail)
        raise HTTPException(status_code=500, detail=f"é¢„æµ‹å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


# --- æ–‡ä»¶ä¸Šä¼ æ¥å£ ---

# åˆ›å»ºä¸Šä¼ ç›®å½•
UPLOAD_DIR = os.path.join(os.path.dirname(__file__), "uploads")
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)


@app.post("/api/upload/dataset")
async def upload_dataset(file: UploadFile = File(...)):
    """ä¸Šä¼ æ•°æ®é›†æ–‡ä»¶"""
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    allowed_extensions = {".csv", ".json"}
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ã€‚è¯·ä¸Šä¼  .csv æˆ– .json æ–‡ä»¶"
        )
    
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_filename = f"{timestamp}_{file.filename}"
    file_path = os.path.join(DATA_DIR, safe_filename)
    
    try:
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # éªŒè¯æ–‡ä»¶å†…å®¹
        if file_ext == ".csv":
            import pandas as pd
            df = pd.read_csv(file_path)
            row_count = len(df)
            columns = list(df.columns)
        elif file_ext == ".json":
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, list):
                row_count = len(data)
                columns = list(data[0].keys()) if data else []
            else:
                row_count = 1
                columns = list(data.keys())
        
        # è¿”å›ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºåç«¯æ ¹ç›®å½•ï¼‰
        relative_path = f"data/{safe_filename}"
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {relative_path}, {row_count} æ¡è®°å½•")
        
        return {
            "status": "success",
            "file_path": relative_path,
            "file_name": safe_filename,
            "original_name": file.filename,
            "file_size": os.path.getsize(file_path),
            "row_count": row_count,
            "columns": columns
        }
        
    except Exception as e:
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œåˆ é™¤å·²ä¸Šä¼ çš„æ–‡ä»¶
        if os.path.exists(file_path):
            os.remove(file_path)
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")


@app.get("/api/datasets")
async def list_datasets():
    """åˆ—å‡ºæ‰€æœ‰å·²ä¸Šä¼ çš„æ•°æ®é›†"""
    datasets = []
    
    for filename in os.listdir(DATA_DIR):
        file_path = os.path.join(DATA_DIR, filename)
        if os.path.isfile(file_path):
            file_ext = os.path.splitext(filename)[1].lower()
            if file_ext in {".csv", ".json"}:
                stat = os.stat(file_path)
                datasets.append({
                    "name": filename,
                    "path": f"data/{filename}",
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "type": file_ext[1:]  # csv æˆ– json
                })
    
    # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
    datasets.sort(key=lambda x: x["modified"], reverse=True)
    return {"datasets": datasets}


@app.delete("/api/datasets/{filename}")
async def delete_dataset(filename: str):
    """åˆ é™¤æ•°æ®é›†æ–‡ä»¶"""
    file_path = os.path.join(DATA_DIR, filename)
    
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    try:
        os.remove(file_path)
        logger.info(f"æ•°æ®é›†å·²åˆ é™¤: {filename}")
        return {"status": "deleted", "filename": filename}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


# --- WebSocket è®­ç»ƒæ—¥å¿—æ¥å£ ---

@app.websocket("/ws/finetune/{task_id}")
async def websocket_finetune_logs(websocket: WebSocket, task_id: str):
    """WebSocket æ¥å£ï¼šå®æ—¶æ¨é€è®­ç»ƒæ—¥å¿—"""
    await ws_manager.connect(websocket, task_id)
    
    try:
        # å‘é€è¿æ¥æˆåŠŸæ¶ˆæ¯
        await websocket.send_json({
            "type": "connected",
            "task_id": task_id,
            "message": "å·²è¿æ¥åˆ°è®­ç»ƒæ—¥å¿—æµ"
        })
        
        # ä¿æŒè¿æ¥ï¼Œæ¥æ”¶å¿ƒè·³æˆ–æ§åˆ¶æ¶ˆæ¯
        while True:
            try:
                data = await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
                # å¤„ç†å¿ƒè·³
                if data == "ping":
                    await websocket.send_text("pong")
            except asyncio.TimeoutError:
                # å‘é€å¿ƒè·³æ£€æµ‹
                try:
                    await websocket.send_json({"type": "heartbeat"})
                except Exception:
                    break
                    
    except WebSocketDisconnect:
        logger.info(f"WebSocket æ–­å¼€: task {task_id}")
    except Exception as e:
        logger.error(f"WebSocket é”™è¯¯: {e}")
    finally:
        ws_manager.disconnect(websocket, task_id)
