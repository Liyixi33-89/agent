#!/usr/bin/env python
"""
æ¨¡å‹é¢„æµ‹å‘½ä»¤è¡Œå·¥å…·

ä½¿ç”¨æ–¹æ³•:
    # å•æ¡æ–‡æœ¬é¢„æµ‹
    python predict.py --model models/my_model.pth --text "è¿™æ˜¯ä¸€æ¡æµ‹è¯•æ–‡æœ¬"
    
    # æ‰¹é‡é¢„æµ‹ï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰
    python predict.py --model models/my_model.pth --input texts.txt --output results.csv
    
    # äº¤äº’å¼é¢„æµ‹
    python predict.py --model models/my_model.pth --interactive
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
    python predict.py --list
"""

import argparse
import os
import sys
import torch
from typing import List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from modeling_bert import load_saved_model, load_tokenizer


def list_available_models(models_dir: str = "models") -> List[dict]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
    if not os.path.exists(models_dir):
        return []
    
    models = []
    for filename in os.listdir(models_dir):
        if filename.endswith('.pth'):
            filepath = os.path.join(models_dir, filename)
            stat = os.stat(filepath)
            
            model_info = {
                "name": filename.replace('.pth', ''),
                "path": filepath,
                "size_mb": round(stat.st_size / (1024 * 1024), 2),
            }
            
            # å°è¯•è¯»å–å…ƒæ•°æ®
            try:
                checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
                if 'config' in checkpoint:
                    model_info['base_model'] = checkpoint['config']._name_or_path
                if 'num_labels' in checkpoint:
                    model_info['num_labels'] = checkpoint['num_labels']
            except:
                pass
            
            models.append(model_info)
    
    return models


def predict_text(
    model, 
    tokenizer, 
    text: str, 
    device: str = 'cpu',
    max_length: int = 512
) -> Tuple[int, float, List[float]]:
    """
    å¯¹å•æ¡æ–‡æœ¬è¿›è¡Œé¢„æµ‹
    
    è¿”å›: (é¢„æµ‹ç±»åˆ«, ç½®ä¿¡åº¦, å„ç±»åˆ«æ¦‚ç‡)
    """
    model.eval()
    
    encoding = tokenizer(
        text,
        truncation=True,
        max_length=max_length,
        padding='max_length',
        return_tensors='pt'
    )
    
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs['logits'] if isinstance(outputs, dict) else outputs
        probabilities = torch.softmax(logits, dim=1)
        prediction = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0][prediction].item()
    
    return prediction, confidence, probabilities[0].tolist()


def predict_batch(
    model, 
    tokenizer, 
    texts: List[str], 
    device: str = 'cpu',
    max_length: int = 512,
    batch_size: int = 16
) -> List[Tuple[int, float, List[float]]]:
    """æ‰¹é‡é¢„æµ‹"""
    results = []
    model.eval()
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        
        encodings = tokenizer(
            batch_texts,
            truncation=True,
            max_length=max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        input_ids = encodings['input_ids'].to(device)
        attention_mask = encodings['attention_mask'].to(device)
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs['logits'] if isinstance(outputs, dict) else outputs
            probabilities = torch.softmax(logits, dim=1)
            predictions = torch.argmax(probabilities, dim=1)
            
            for j in range(len(batch_texts)):
                pred = predictions[j].item()
                conf = probabilities[j][pred].item()
                probs = probabilities[j].tolist()
                results.append((pred, conf, probs))
    
    return results


def interactive_mode(model, tokenizer, device: str = 'cpu', label_names: List[str] = None):
    """äº¤äº’å¼é¢„æµ‹æ¨¡å¼"""
    print("\n" + "=" * 50)
    print("ğŸ¤– äº¤äº’å¼é¢„æµ‹æ¨¡å¼")
    print("=" * 50)
    print("è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n")
    
    while True:
        try:
            text = input("ğŸ“ è¯·è¾“å…¥æ–‡æœ¬: ").strip()
            
            if text.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if not text:
                print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬\n")
                continue
            
            prediction, confidence, probabilities = predict_text(model, tokenizer, text, device)
            
            # æ˜¾ç¤ºç»“æœ
            print("\n" + "-" * 40)
            if label_names and prediction < len(label_names):
                print(f"ğŸ“Š é¢„æµ‹ç±»åˆ«: {prediction} ({label_names[prediction]})")
            else:
                print(f"ğŸ“Š é¢„æµ‹ç±»åˆ«: {prediction}")
            print(f"âœ… ç½®ä¿¡åº¦: {confidence:.2%}")
            print(f"ğŸ“ˆ å„ç±»åˆ«æ¦‚ç‡: {[f'{p:.4f}' for p in probabilities]}")
            print("-" * 40 + "\n")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é¢„æµ‹å‡ºé”™: {e}\n")


def main():
    parser = argparse.ArgumentParser(
        description="æ¨¡å‹é¢„æµ‹å‘½ä»¤è¡Œå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        '--model', '-m',
        type=str,
        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚: models/my_model.pth'
    )
    
    parser.add_argument(
        '--base-model', '-b',
        type=str,
        default='bert-base-uncased',
        help='åŸºç¡€æ¨¡å‹åç§°ï¼ˆç”¨äºåŠ è½½åˆ†è¯å™¨ï¼‰ï¼Œé»˜è®¤: bert-base-uncased'
    )
    
    parser.add_argument(
        '--text', '-t',
        type=str,
        help='è¦é¢„æµ‹çš„æ–‡æœ¬'
    )
    
    parser.add_argument(
        '--input', '-i',
        type=str,
        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€æ¡æ–‡æœ¬ï¼‰'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆCSV æ ¼å¼ï¼‰'
    )
    
    parser.add_argument(
        '--interactive',
        action='store_true',
        help='å¯åŠ¨äº¤äº’å¼é¢„æµ‹æ¨¡å¼'
    )
    
    parser.add_argument(
        '--list', '-l',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹'
    )
    
    parser.add_argument(
        '--device', '-d',
        type=str,
        default='auto',
        choices=['auto', 'cpu', 'cuda'],
        help='è®¡ç®—è®¾å¤‡ï¼Œé»˜è®¤: autoï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰'
    )
    
    parser.add_argument(
        '--labels',
        type=str,
        nargs='+',
        help='ç±»åˆ«æ ‡ç­¾åç§°ï¼Œä¾‹å¦‚: --labels è´Ÿé¢ æ­£é¢'
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    if args.list:
        models = list_available_models()
        if not models:
            print("âš ï¸ models/ ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
            print("ğŸ’¡ æç¤º: è¯·å…ˆå®Œæˆä¸€æ¬¡å¾®è°ƒä»»åŠ¡ä»¥ç”Ÿæˆæ¨¡å‹")
        else:
            print("\nğŸ“¦ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
            print("-" * 60)
            for m in models:
                print(f"  â€¢ {m['name']}")
                print(f"    è·¯å¾„: {m['path']}")
                print(f"    å¤§å°: {m['size_mb']} MB")
                if 'base_model' in m:
                    print(f"    åŸºç¡€æ¨¡å‹: {m['base_model']}")
                if 'num_labels' in m:
                    print(f"    ç±»åˆ«æ•°: {m['num_labels']}")
                print()
        return
    
    # æ£€æŸ¥å¿…è¦å‚æ•°
    if not args.model:
        parser.error("è¯·æŒ‡å®šæ¨¡å‹æ–‡ä»¶è·¯å¾„: --model models/xxx.pth")
    
    if not os.path.exists(args.model):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        print("ğŸ’¡ ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
        sys.exit(1)
    
    # ç¡®å®šè®¾å¤‡
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    print(f"\nğŸ”§ åŠ è½½æ¨¡å‹: {args.model}")
    print(f"ğŸ’» è®¡ç®—è®¾å¤‡: {device.upper()}")
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    try:
        # å…ˆè¯»å–æ¨¡å‹å…ƒæ•°æ®ä»¥è·å– base_model
        checkpoint = torch.load(args.model, map_location='cpu', weights_only=False)
        if 'config' in checkpoint and hasattr(checkpoint['config'], '_name_or_path'):
            base_model = checkpoint['config']._name_or_path
            print(f"ğŸ“Œ æ£€æµ‹åˆ°åŸºç¡€æ¨¡å‹: {base_model}")
        else:
            base_model = args.base_model
        
        tokenizer = load_tokenizer(base_model)
        model = load_saved_model(args.model, device=device)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    # äº¤äº’å¼æ¨¡å¼
    if args.interactive:
        interactive_mode(model, tokenizer, device, args.labels)
        return
    
    # å•æ¡æ–‡æœ¬é¢„æµ‹
    if args.text:
        prediction, confidence, probabilities = predict_text(model, tokenizer, args.text, device)
        
        print("=" * 50)
        print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: {args.text}")
        print("-" * 50)
        
        if args.labels and prediction < len(args.labels):
            print(f"ğŸ“Š é¢„æµ‹ç±»åˆ«: {prediction} ({args.labels[prediction]})")
        else:
            print(f"ğŸ“Š é¢„æµ‹ç±»åˆ«: {prediction}")
        
        print(f"âœ… ç½®ä¿¡åº¦: {confidence:.2%}")
        print(f"ğŸ“ˆ å„ç±»åˆ«æ¦‚ç‡: {[f'{p:.4f}' for p in probabilities]}")
        print("=" * 50)
        return
    
    # æ‰¹é‡é¢„æµ‹ï¼ˆä»æ–‡ä»¶ï¼‰
    if args.input:
        if not os.path.exists(args.input):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            sys.exit(1)
        
        # è¯»å–è¾“å…¥æ–‡ä»¶
        with open(args.input, 'r', encoding='utf-8') as f:
            texts = [line.strip() for line in f if line.strip()]
        
        print(f"ğŸ“„ è¯»å– {len(texts)} æ¡æ–‡æœ¬")
        print("ğŸ”„ å¼€å§‹æ‰¹é‡é¢„æµ‹...")
        
        results = predict_batch(model, tokenizer, texts, device)
        
        # è¾“å‡ºç»“æœ
        output_file = args.output or args.input.replace('.txt', '_results.csv')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("text,prediction,confidence,probabilities\n")
            for text, (pred, conf, probs) in zip(texts, results):
                # è½¬ä¹‰æ–‡æœ¬ä¸­çš„é€—å·å’Œå¼•å·
                text_escaped = text.replace('"', '""')
                probs_str = ';'.join([f'{p:.4f}' for p in probs])
                f.write(f'"{text_escaped}",{pred},{conf:.4f},"{probs_str}"\n')
        
        print(f"âœ… é¢„æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        predictions = [r[0] for r in results]
        for label in set(predictions):
            count = predictions.count(label)
            pct = count / len(predictions) * 100
            label_name = f" ({args.labels[label]})" if args.labels and label < len(args.labels) else ""
            print(f"   ç±»åˆ« {label}{label_name}: {count} æ¡ ({pct:.1f}%)")
        
        return
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ“ä½œï¼Œæ˜¾ç¤ºå¸®åŠ©
    parser.print_help()


if __name__ == "__main__":
    main()
