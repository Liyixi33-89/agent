import torch
import torch.nn as nn
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import DataLoader
from typing import List, Dict, Any, Optional, Callable
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import os
import gc

def clear_gpu_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

class Trainer:
    """æ¨¡å‹è®­ç»ƒå™¨ - æ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œæ˜¾å­˜ä¼˜åŒ–"""
    
    def __init__(self, model: nn.Module, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        self.model = model
        self.device = device
        self.model.to(device)
        
        # å¦‚æœä½¿ç”¨GPUï¼Œå¯ç”¨å†…å­˜ä¼˜åŒ–
        if device == "cuda":
            torch.backends.cudnn.benchmark = True
    
    def train_epoch(self, train_loader: DataLoader, optimizer: torch.optim.Optimizer, 
                   scheduler: Optional[Any] = None, 
                   gradient_accumulation_steps: int = 1,
                   use_amp: bool = False,
                   scaler: Optional[Any] = None) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch - æ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒ"""
        self.model.train()
        total_loss = 0
        all_preds = []
        all_labels = []
        
        optimizer.zero_grad()  # åœ¨epochå¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
        
        for batch_idx, batch in enumerate(train_loader):
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if use_amp and scaler is not None:
                with torch.cuda.amp.autocast():
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs['loss'] / gradient_accumulation_steps
                scaler.scale(loss).backward()
            else:
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs['loss'] / gradient_accumulation_steps
                # åå‘ä¼ æ’­
                loss.backward()
            
            # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨åŸå§‹losså€¼ï¼‰
            total_loss += loss.item() * gradient_accumulation_steps
            
            # è·å–é¢„æµ‹ç»“æœ
            with torch.no_grad():
                preds = torch.argmax(outputs['logits'], dim=1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯ gradient_accumulation_steps æ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                if use_amp and scaler is not None:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                
                if scheduler:
                    scheduler.step()
                optimizer.zero_grad()
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if (batch_idx + 1) % (gradient_accumulation_steps * 10) == 0:
                    clear_gpu_memory()
        
        # å¤„ç†å‰©ä½™çš„æ¢¯åº¦
        if (batch_idx + 1) % gradient_accumulation_steps != 0:
            if use_amp and scaler is not None:
                scaler.step(optimizer)
                scaler.update()
            else:
                optimizer.step()
            optimizer.zero_grad()
        
        # è®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(train_loader)
        accuracy = accuracy_score(all_labels, all_preds)
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy
        }
    
    def evaluate(self, val_loader: DataLoader) -> Dict[str, float]:
        """æ¨¡å‹è¯„ä¼°"""
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs['loss']
                
                total_loss += loss.item()
                
                preds = torch.argmax(outputs['logits'], dim=1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è¯„ä¼°åæ¸…ç†æ˜¾å­˜
        clear_gpu_memory()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = accuracy_score(all_labels, all_preds)
        precision = precision_score(all_labels, all_preds, average='weighted')
        recall = recall_score(all_labels, all_preds, average='weighted')
        f1 = f1_score(all_labels, all_preds, average='weighted')
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }
    
    def predict(self, texts: List[str], tokenizer, max_length: int = 128) -> List[int]:
        """é¢„æµ‹"""
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            for text in texts:
                # ç¼–ç æ–‡æœ¬
                encoding = tokenizer(
                    text,
                    truncation=True,
                    padding='max_length',
                    max_length=max_length,
                    return_tensors='pt'
                )
                
                input_ids = encoding['input_ids'].to(self.device)
                attention_mask = encoding['attention_mask'].to(self.device)
                
                # é¢„æµ‹
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                pred = torch.argmax(outputs['logits'], dim=1)
                predictions.append(pred.cpu().item())
        
        return predictions

def train_model(
    model: nn.Module, 
    train_loader: DataLoader, 
    val_loader: DataLoader, 
    epochs: int = 3, 
    learning_rate: float = 2e-5, 
    warmup_steps: int = 0,
    progress_callback: Optional[Callable[[int, int, float], None]] = None,
    device: str = None,
    gradient_accumulation_steps: int = 4,  # æ–°å¢ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    use_amp: bool = True  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
) -> Dict[str, Any]:
    """
    è®­ç»ƒæ¨¡å‹ - æ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒ
    
    Args:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        epochs: è®­ç»ƒè½®æ•°
        learning_rate: å­¦ä¹ ç‡
        warmup_steps: é¢„çƒ­æ­¥æ•°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œå‚æ•°ä¸º (å½“å‰epoch, æ€»epochs, è¿›åº¦ç™¾åˆ†æ¯”)
        device: è®¾å¤‡ç±»å‹ï¼Œcuda æˆ– cpuï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
        gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç”¨äºå‡å°‘æ˜¾å­˜å ç”¨
        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä»…GPUæœ‰æ•ˆï¼‰
    """
    # è®¾å¤‡é€‰æ‹©: ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ deviceï¼Œå¦åˆ™è‡ªåŠ¨æ£€æµ‹
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æ¸…ç†æ˜¾å­˜åå†å¼€å§‹è®­ç»ƒ
    clear_gpu_memory()
    
    print(f"ğŸ’» è®­ç»ƒè®¾å¤‡: {device.upper()}")
    if device == "cuda":
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   æ˜¾å­˜: {total_mem:.1f} GB")
        print(f"   æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps} æ­¥ (ç­‰æ•ˆbatch_size: {train_loader.batch_size * gradient_accumulation_steps})")
        if use_amp:
            print(f"   æ··åˆç²¾åº¦è®­ç»ƒ: å·²å¯ç”¨ (FP16)")
    else:
        use_amp = False  # CPUä¸æ”¯æŒAMP
    
    trainer = Trainer(model, device)
    
    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    
    # æ··åˆç²¾åº¦è®­ç»ƒscaler
    scaler = torch.cuda.amp.GradScaler() if (use_amp and device == "cuda") else None
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆè€ƒè™‘æ¢¯åº¦ç´¯ç§¯ï¼‰
    total_steps = (len(train_loader) // gradient_accumulation_steps) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'train_accuracy': [],
        'val_loss': [],
        'val_accuracy': [],
        'val_precision': [],
        'val_recall': [],
        'val_f1': []
    }
    
    best_val_f1 = 0
    best_model_state = None
    
    try:
        for epoch in range(epochs):
            print(f"\nEpoch {epoch + 1}/{epochs}")
            
            # è®¡ç®—å½“å‰è¿›åº¦å¹¶è°ƒç”¨å›è°ƒ
            progress = ((epoch + 0.5) / epochs) * 100  # è®­ç»ƒä¸­é—´ç‚¹
            if progress_callback:
                try:
                    progress_callback(epoch + 1, epochs, progress)
                except Exception as e:
                    print(f"Progress callback error: {e}")
            
            # è®­ç»ƒï¼ˆä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦ï¼‰
            train_metrics = trainer.train_epoch(
                train_loader, optimizer, scheduler,
                gradient_accumulation_steps=gradient_accumulation_steps,
                use_amp=use_amp,
                scaler=scaler
            )
            history['train_loss'].append(train_metrics['loss'])
            history['train_accuracy'].append(train_metrics['accuracy'])
            
            # éªŒè¯
            val_metrics = trainer.evaluate(val_loader)
            history['val_loss'].append(val_metrics['loss'])
            history['val_accuracy'].append(val_metrics['accuracy'])
            history['val_precision'].append(val_metrics['precision'])
            history['val_recall'].append(val_metrics['recall'])
            history['val_f1'].append(val_metrics['f1_score'])
            
            print(f"Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['accuracy']:.4f}")
            print(f"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}")
            print(f"Val F1: {val_metrics['f1_score']:.4f}")
            
            # æ˜¾ç¤ºGPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            if device == "cuda":
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                reserved = torch.cuda.memory_reserved(0) / 1024**3
                print(f"GPUæ˜¾å­˜: å·²åˆ†é… {allocated:.2f}GB / å·²ä¿ç•™ {reserved:.2f}GB")
            
            # æ¯ä¸ªepochç»“æŸåæ›´æ–°è¿›åº¦
            progress = ((epoch + 1) / epochs) * 100
            if progress_callback:
                try:
                    progress_callback(epoch + 1, epochs, progress)
                except Exception as e:
                    print(f"Progress callback error: {e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['f1_score'] > best_val_f1:
                best_val_f1 = val_metrics['f1_score']
                best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            
            # æ¯ä¸ªepochåæ¸…ç†æ˜¾å­˜
            clear_gpu_memory()
            
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"\nâŒ GPUæ˜¾å­˜ä¸è¶³ï¼å»ºè®®:")
            print(f"   1. å‡å° batch_size (å½“å‰: {train_loader.batch_size})")
            print(f"   2. å‡å° max_length")
            print(f"   3. å¢å¤§ gradient_accumulation_steps (å½“å‰: {gradient_accumulation_steps})")
            clear_gpu_memory()
        raise e
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state:
        model.load_state_dict(best_model_state)
    
    # æœ€ç»ˆæ¸…ç†
    clear_gpu_memory()
    
    return history