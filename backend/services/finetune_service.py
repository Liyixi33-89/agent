"""
å¾®è°ƒä»»åŠ¡æœåŠ¡å±‚
å¤„ç†å¾®è°ƒä»»åŠ¡çš„æ ¸å¿ƒé€»è¾‘
"""
import os
import logging
import threading
from typing import Dict

from schemas.requests import FinetuneRequest
from db_models import TaskStatus
import crud

logger = logging.getLogger(__name__)

# å­˜å‚¨è¿è¡Œä¸­çš„ä»»åŠ¡çº¿ç¨‹ï¼Œç”¨äºå–æ¶ˆåŠŸèƒ½
running_tasks: Dict[str, threading.Thread] = {}
task_cancel_flags: Dict[str, bool] = {}


def run_finetune_task_sync(task_id: str, req: FinetuneRequest):
    """åŒæ­¥è¿è¡Œå¾®è°ƒä»»åŠ¡"""
    import torch
    from database import SessionLocal
    from utils_data import load_csv_data, load_json_data, create_data_loader, split_data
    from modeling_bert import load_model, load_tokenizer, save_model
    from trainer import train_model
    
    # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯ï¼ˆå› ä¸ºåœ¨çº¿ç¨‹ä¸­ï¼‰
    db = SessionLocal()
    
    # åˆå§‹åŒ–å–æ¶ˆæ ‡å¿—
    task_cancel_flags[task_id] = False
    
    # æ ¹æ®é…ç½®å’Œç¡¬ä»¶æƒ…å†µå†³å®šä½¿ç”¨çš„è®¾å¤‡
    if req.use_gpu and torch.cuda.is_available():
        device = "cuda"
        logger.info(f"ğŸš€ ä½¿ç”¨ GPU è®­ç»ƒ: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        if req.use_gpu and not torch.cuda.is_available():
            logger.warning("âš ï¸ è¯·æ±‚ä½¿ç”¨ GPU ä½† CUDA ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU è®­ç»ƒ")
        else:
            logger.info("ğŸ“Œ ä½¿ç”¨ CPU è®­ç»ƒ")
    
    # è¿›åº¦å›è°ƒå‡½æ•°
    def progress_callback(current_epoch: int, total_epochs: int, progress: float):
        """æ›´æ–°è®­ç»ƒè¿›åº¦åˆ°æ•°æ®åº“"""
        # æ£€æŸ¥å–æ¶ˆæ ‡å¿—
        if task_cancel_flags.get(task_id, False):
            raise InterruptedError("ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ")
        
        try:
            crud.update_finetune_task_status(
                db=db,
                task_id=task_id,
                status=TaskStatus.RUNNING.value,
                progress=progress
            )
            logger.info(f"Task {task_id}: Epoch {current_epoch}/{total_epochs}, Progress: {progress:.1f}%")
        except Exception as e:
            logger.error(f"Error updating progress: {e}")
    
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        crud.update_finetune_task_status(db, task_id, TaskStatus.RUNNING.value, progress=0.0)
        
        logger.info(f"Starting finetune task {task_id} for model {req.new_model_name}...")
        
        # åŠ è½½æ•°æ®
        if req.dataset_path.endswith('.csv'):
            texts, labels = load_csv_data(req.dataset_path, req.text_column, req.label_column)
        elif req.dataset_path.endswith('.json'):
            texts, labels = load_json_data(req.dataset_path, req.text_column, req.label_column)
        else:
            raise ValueError("Unsupported file format. Use CSV or JSON.")
        
        # åˆ’åˆ†æ•°æ®é›†
        train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = split_data(
            texts, labels, train_ratio=0.8, val_ratio=0.1
        )
        
        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        tokenizer = load_tokenizer(req.base_model)
        model = load_model(req.base_model, num_labels=len(set(labels)))
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = create_data_loader(
            train_texts, train_labels, tokenizer,
            batch_size=req.batch_size, max_length=req.max_length, shuffle=True
        )
        val_loader = create_data_loader(
            val_texts, val_labels, tokenizer,
            batch_size=req.batch_size, max_length=req.max_length
        )
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦è¿›åº¦å›è°ƒã€è®¾å¤‡é…ç½®å’Œæ˜¾å­˜ä¼˜åŒ–ï¼‰
        history = train_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=req.epochs,
            learning_rate=req.learning_rate,
            progress_callback=progress_callback,
            device=device,
            gradient_accumulation_steps=req.gradient_accumulation_steps,
            use_amp=(device == "cuda")
        )
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/{req.new_model_name}.pth"
        os.makedirs("models", exist_ok=True)
        save_model(model, model_path)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.COMPLETED.value,
            model_path=model_path,
            training_history=history,
            progress=100.0
        )
        
        # åˆ›å»ºæ¨¡å‹è®°å½•
        crud.create_model(
            db=db,
            name=req.new_model_name,
            model_type="finetuned",
            base_model=req.base_model,
            path=model_path,
            description=f"ä» {req.base_model} å¾®è°ƒå¾—åˆ°",
            finetune_task_id=task_id
        )
        
        logger.info(f"Finetune task {task_id} completed. Model saved to {model_path}")
        
    except InterruptedError as e:
        # ä»»åŠ¡è¢«å–æ¶ˆ
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.CANCELLED.value,
            error_message=str(e)
        )
        logger.warning(f"Finetune task {task_id} cancelled: {str(e)}")
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        import traceback
        error_detail = f"{str(e)}\n{traceback.format_exc()}"
        crud.update_finetune_task_status(
            db=db,
            task_id=task_id,
            status=TaskStatus.FAILED.value,
            error_message=error_detail
        )
        logger.error(f"Finetune task {task_id} failed: {str(e)}")
    finally:
        # æ¸…ç†å–æ¶ˆæ ‡å¿—
        if task_id in task_cancel_flags:
            del task_cancel_flags[task_id]
        if task_id in running_tasks:
            del running_tasks[task_id]
        db.close()


async def run_finetune_task(task_id: str, req: FinetuneRequest):
    """å¼‚æ­¥è¿è¡Œå¾®è°ƒä»»åŠ¡"""
    # åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ä»»åŠ¡
    thread = threading.Thread(target=run_finetune_task_sync, args=(task_id, req))
    thread.start()
    # è®°å½•è¿è¡Œä¸­çš„ä»»åŠ¡
    running_tasks[task_id] = thread
